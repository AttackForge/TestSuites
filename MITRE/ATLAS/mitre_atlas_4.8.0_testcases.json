[
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Create Proxy ML Model",
                "details": "<p>Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.<br/>Proxy models are used to simulate complete access to the target model in a fully offline manner.</p><p>Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.</p>"
            },
            {
                "title": "Journals and Conference Proceedings",
                "details": "<p>Many of the publications accepted at premier machine learning conferences and journals come from commercial labs.<br/>Some journals and conferences are open access, others may require paying for access or a membership.<br/>These publications will often describe in detail all aspects of a particular approach for reproducibility.<br/>This information can be used by adversaries to implement the paper.</p>"
            },
            {
                "title": "Pre-Print Repositories",
                "details": "<p>Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed.<br/>They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings.<br/>Pre-print repositories also serve as a central location to share papers that have been accepted to journals.<br/>Searching pre-print repositories  provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on.</p>"
            },
            {
                "title": "Technical Blogs",
                "details": "<p>Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems.<br/>Individual researchers also frequently document their work in blogposts.<br/>An adversary may search for posts made by the target victim organization or its employees.<br/>In comparison to Journals and Conference Proceedings and Pre-Print Repositories this material will often contain more practical aspects of the machine learning system.<br/>This could include underlying technologies and frameworks used, and possibly some information about the API access and use case.<br/>This will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/",
            "Mitigation Name: Limit Public Release of Information",
            "Mitigation Id: AML.M0000",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation Category: Policy"
        ],
        "testcase": "Search for Victim's Publicly Available Research Materials",
        "code": "AML.T0000",
        "details": "<p>Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.<br/>The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.<br/>Organizations often use open source model architectures trained on additional proprietary data in production.<br/>Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models (Create Proxy ML Model).<br/>An adversary can search these resources for publications for authors employed at the victim organization.</p><p>Research materials may exist as academic papers published in Journals and Conference Proceedings, or stored in Pre-Print Repositories, as well as Technical Blogs.</p><p><b>MITIGATION</b>: Limit the public release of technical information about the machine learning stack used in an organization's products or services. Technical knowledge of how machine learning is used can be leveraged by adversaries to perform targeting and tailor attacks to the target system. Additionally, consider limiting the release of organizational information - including physical locations, researcher names, and department structures - from which technical details such as machine learning techniques, model architectures, or datasets may be inferred.</p><p>Limit the connection between publicly disclosed approaches and the data, models, and algorithms used in production.</p>",
        "sort_order": 1
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Search for Victim's Publicly Available Research Materials",
            "Subtechnique of Id: AML.T0000",
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/"
        ],
        "testcase": "Journals and Conference Proceedings",
        "code": "AML.T0000.000",
        "details": "<p>Many of the publications accepted at premier machine learning conferences and journals come from commercial labs.<br/>Some journals and conferences are open access, others may require paying for access or a membership.<br/>These publications will often describe in detail all aspects of a particular approach for reproducibility.<br/>This information can be used by adversaries to implement the paper.</p>",
        "sort_order": 2
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Search for Victim's Publicly Available Research Materials",
            "Subtechnique of Id: AML.T0000",
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/"
        ],
        "testcase": "Pre-Print Repositories",
        "code": "AML.T0000.001",
        "details": "<p>Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed.<br/>They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings.<br/>Pre-print repositories also serve as a central location to share papers that have been accepted to journals.<br/>Searching pre-print repositories  provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on.</p>",
        "sort_order": 3
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Journals and Conference Proceedings",
                "details": "<p>Many of the publications accepted at premier machine learning conferences and journals come from commercial labs.<br/>Some journals and conferences are open access, others may require paying for access or a membership.<br/>These publications will often describe in detail all aspects of a particular approach for reproducibility.<br/>This information can be used by adversaries to implement the paper.</p>"
            },
            {
                "title": "Pre-Print Repositories",
                "details": "<p>Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed.<br/>They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings.<br/>Pre-print repositories also serve as a central location to share papers that have been accepted to journals.<br/>Searching pre-print repositories  provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Search for Victim's Publicly Available Research Materials",
            "Subtechnique of Id: AML.T0000",
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/"
        ],
        "testcase": "Technical Blogs",
        "code": "AML.T0000.002",
        "details": "<p>Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems.<br/>Individual researchers also frequently document their work in blogposts.<br/>An adversary may search for posts made by the target victim organization or its employees.<br/>In comparison to Journals and Conference Proceedings and Pre-Print Repositories this material will often contain more practical aspects of the machine learning system.<br/>This could include underlying technologies and frameworks used, and possibly some information about the API access and use case.<br/>This will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack.</p>",
        "sort_order": 4
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Search for Victim's Publicly Available Research Materials",
                "details": "<p>Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.<br/>The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.<br/>Organizations often use open source model architectures trained on additional proprietary data in production.<br/>Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models (Create Proxy ML Model).<br/>An adversary can search these resources for publications for authors employed at the victim organization.</p><p>Research materials may exist as academic papers published in Journals and Conference Proceedings, or stored in Pre-Print Repositories, as well as Technical Blogs.</p>"
            },
            {
                "title": "Adversarial ML Attack Implementations",
                "details": "<p>Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack.</p>"
            },
            {
                "title": "Adversarial ML Attacks",
                "details": "<p>Adversaries may develop their own adversarial attacks.<br/>They may leverage existing libraries as a starting point (Adversarial ML Attack Implementations).<br/>They may implement ideas described in public research papers or develop custom made attacks for the victim model.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/"
        ],
        "testcase": "Search for Publicly Available Adversarial Vulnerability Analysis",
        "code": "AML.T0001",
        "details": "<p>Much like the Search for Victim's Publicly Available Research Materials, there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.<br/>This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may obtain Adversarial ML Attack Implementations or develop their own Adversarial ML Attacks if necessary.</p>",
        "sort_order": 5
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Adversarial ML Attacks",
                "details": "<p>Adversaries may develop their own adversarial attacks.<br/>They may leverage existing libraries as a starting point (Adversarial ML Attack Implementations).<br/>They may implement ideas described in public research papers or develop custom made attacks for the victim model.</p>"
            },
            {
                "title": "Manual Modification",
                "details": "<p>Adversaries may manually modify the input data to craft adversarial data.<br/>They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task.<br/>The adversary may use trial and error until they are able to verify they have a working adversarial input.</p>"
            },
            {
                "title": "Search for Victim's Publicly Available Research Materials",
                "details": "<p>Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.<br/>The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.<br/>Organizations often use open source model architectures trained on additional proprietary data in production.<br/>Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models (Create Proxy ML Model).<br/>An adversary can search these resources for publications for authors employed at the victim organization.</p><p>Research materials may exist as academic papers published in Journals and Conference Proceedings, or stored in Pre-Print Repositories, as well as Technical Blogs.</p>"
            },
            {
                "title": "Search for Publicly Available Adversarial Vulnerability Analysis",
                "details": "<p>Much like the Search for Victim's Publicly Available Research Materials, there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.<br/>This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may obtain Adversarial ML Attack Implementations or develop their own Adversarial ML Attacks if necessary.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/",
            "Mitigation Name: Limit Public Release of Information",
            "Mitigation Id: AML.M0000",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation Category: Policy"
        ],
        "testcase": "Search Victim-Owned Websites",
        "code": "AML.T0003",
        "details": "<p>Adversaries may search websites owned by the victim for information that can be used during targeting.<br/>Victim-owned websites may contain technical details about their ML-enabled products or services.<br/>Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.<br/>These sites may also have details highlighting business operations and relationships.</p><p>Adversaries may search victim-owned websites to gather actionable information.<br/>This information may help adversaries tailor their attacks (e.g. Adversarial ML Attacks or Manual Modification).<br/>Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. Search for Victim's Publicly Available Research Materials or Search for Publicly Available Adversarial Vulnerability Analysis)</p><p><b>MITIGATION</b>: Limit the public release of technical information about the machine learning stack used in an organization's products or services. Technical knowledge of how machine learning is used can be leveraged by adversaries to perform targeting and tailor attacks to the target system. Additionally, consider limiting the release of organizational information - including physical locations, researcher names, and department structures - from which technical details such as machine learning techniques, model architectures, or datasets may be inferred.</p><p>Restrict release of technical information on ML-enabled products and organizational information on the teams supporting ML-enabled products.</p>",
        "sort_order": 6
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Acquire Public ML Artifacts",
                "details": "<p>Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.<br/>These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.<br/>An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.<br/>Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. Search Victim-Owned Websites or Search for Victim's Publicly Available Research Materials).<br/>These ML artifacts often provide adversaries with details of the ML task and approach.</p><p>ML artifacts can aid in an adversary's ability to Create Proxy ML Model.<br/>If these artifacts include pieces of the actual model in production, they can be used to directly Craft Adversarial Data.<br/>Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to Establish Accounts.</p><p>Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/",
            "Mitigation Name: Limit Public Release of Information",
            "Mitigation Id: AML.M0000",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation Category: Policy"
        ],
        "testcase": "Search Application Repositories",
        "code": "AML.T0004",
        "details": "<p>Adversaries may search open application repositories during targeting.<br/>Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store.</p><p>Adversaries may craft search queries seeking applications that contain a ML-enabled components.<br/>Frequently, the next step is to Acquire Public ML Artifacts.</p><p><b>MITIGATION</b>: Limit the public release of technical information about the machine learning stack used in an organization's products or services. Technical knowledge of how machine learning is used can be leveraged by adversaries to perform targeting and tailor attacks to the target system. Additionally, consider limiting the release of organizational information - including physical locations, researcher names, and department structures - from which technical details such as machine learning techniques, model architectures, or datasets may be inferred.</p><p>Limit the release of sensitive information in the metadata of deployed systems and publicly available applications.</p>",
        "sort_order": 7
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/"
        ],
        "testcase": "Active Scanning",
        "code": "AML.T0006",
        "details": "<p>An adversary may probe or scan the victim system to gather information for targeting.<br/>This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.</p>",
        "sort_order": 8
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Search Victim-Owned Websites",
                "details": "<p>Adversaries may search websites owned by the victim for information that can be used during targeting.<br/>Victim-owned websites may contain technical details about their ML-enabled products or services.<br/>Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info.<br/>These sites may also have details highlighting business operations and relationships.</p><p>Adversaries may search victim-owned websites to gather actionable information.<br/>This information may help adversaries tailor their attacks (e.g. Adversarial ML Attacks or Manual Modification).<br/>Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. Search for Victim's Publicly Available Research Materials or Search for Publicly Available Adversarial Vulnerability Analysis)</p>"
            },
            {
                "title": "Search for Victim's Publicly Available Research Materials",
                "details": "<p>Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization.<br/>The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective.<br/>Organizations often use open source model architectures trained on additional proprietary data in production.<br/>Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models (Create Proxy ML Model).<br/>An adversary can search these resources for publications for authors employed at the victim organization.</p><p>Research materials may exist as academic papers published in Journals and Conference Proceedings, or stored in Pre-Print Repositories, as well as Technical Blogs.</p>"
            },
            {
                "title": "Create Proxy ML Model",
                "details": "<p>Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.<br/>Proxy models are used to simulate complete access to the target model in a fully offline manner.</p><p>Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.</p>"
            },
            {
                "title": "Craft Adversarial Data",
                "details": "<p>Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.<br/>Effects can range from misclassification, to missed detections, to maximizing energy consumption.<br/>Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.<br/>For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.</p><p>Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization, Black-Box Optimization, Black-Box Transfer, or Manual Modification.</p><p>The adversary may Verify Attack their approach works if they have white-box or inference API access to the model.<br/>This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.<br/>They can then use the attack at a later time to accomplish their goals.<br/>An adversary may optimize adversarial examples for Evade ML Model, or to Erode ML Model Integrity.</p>"
            },
            {
                "title": "Establish Accounts",
                "details": "<p>Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging, or for victim impersonation.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/",
            "Mitigation Name: Limit Public Release of Information",
            "Mitigation Id: AML.M0000",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation Category: Policy"
        ],
        "testcase": "Acquire Public ML Artifacts",
        "code": "AML.T0002",
        "details": "<p>Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts.<br/>These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters.<br/>An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment.<br/>Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. Search Victim-Owned Websites or Search for Victim's Publicly Available Research Materials).<br/>These ML artifacts often provide adversaries with details of the ML task and approach.</p><p>ML artifacts can aid in an adversary's ability to Create Proxy ML Model.<br/>If these artifacts include pieces of the actual model in production, they can be used to directly Craft Adversarial Data.<br/>Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to Establish Accounts.</p><p>Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.</p><p><b>MITIGATION</b>: Limit the public release of technical information about the machine learning stack used in an organization's products or services. Technical knowledge of how machine learning is used can be leveraged by adversaries to perform targeting and tailor attacks to the target system. Additionally, consider limiting the release of organizational information - including physical locations, researcher names, and department structures - from which technical details such as machine learning techniques, model architectures, or datasets may be inferred.</p><p>Limit the release of sensitive information in the metadata of deployed systems and publicly available applications.</p>",
        "sort_order": 9
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Establish Accounts",
                "details": "<p>Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging, or for victim impersonation.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Acquire Public ML Artifacts",
            "Subtechnique of Id: AML.T0002",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/",
            "Mitigation Name: Limit Model Artifact Release",
            "Mitigation Id: AML.M0001",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Policy"
        ],
        "testcase": "Datasets",
        "code": "AML.T0002.000",
        "details": "<p>Adversaries may collect public datasets to use in their operations.<br/>Datasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries.<br/>Datasets can be stored in cloud storage, or on victim-owned websites.<br/>Some datasets require the adversary to Establish Accounts for access.</p><p>Acquired datasets help the adversary advance their operations, stage attacks,  and tailor attacks to the victim organization.</p><p><b>MITIGATION</b>: Limit public release of technical project details including data, algorithms, model architectures, and model checkpoints that are used in production, or that are representative of those used in production.</p><p>Limiting the release of datasets can reduce an adversary's ability to target production models trained on the same or similar data.</p>",
        "sort_order": 10
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Acquire Public ML Artifacts",
            "Subtechnique of Id: AML.T0002",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/",
            "Mitigation Name: Limit Model Artifact Release",
            "Mitigation Id: AML.M0001",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Policy"
        ],
        "testcase": "Models",
        "code": "AML.T0002.001",
        "details": "<p>Adversaries may acquire public models to use in their operations.<br/>Adversaries may seek models used by the victim organization or models that are representative of those used by the victim organization.<br/>Representative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset.<br/>The adversary may search public sources for common model architecture configuration file formats such as YAML or Python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite).</p><p>Acquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model.</p><p><b>MITIGATION</b>: Limit public release of technical project details including data, algorithms, model architectures, and model checkpoints that are used in production, or that are representative of those used in production.</p><p>Limiting the release of model architectures and checkpoints can reduce an adversary's ability to target those models.</p>",
        "sort_order": 11
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Adversarial ML Attack Implementations",
                "details": "<p>Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack.</p>"
            },
            {
                "title": "Software Tools",
                "details": "<p>Adversaries may search for and obtain software tools to support their operations.<br/>Software designed for legitimate use may be repurposed by an adversary for malicious intent.<br/>An adversary may modify or customize software tools to achieve their purpose.<br/>Software tools used to support attacks on ML systems are not necessarily ML-based themselves.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Obtain Capabilities",
        "code": "AML.T0016",
        "details": "<p>Adversaries may search for and obtain software capabilities for use in their operations.<br/>Capabilities may be specific to ML-based attacks Adversarial ML Attack Implementations or generic software tools repurposed for malicious intent (Software Tools). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.</p>",
        "sort_order": 12
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Obtain Capabilities",
            "Subtechnique of Id: AML.T0016",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Adversarial ML Attack Implementations",
        "code": "AML.T0016.000",
        "details": "<p>Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack.</p>",
        "sort_order": 13
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Obtain Capabilities",
            "Subtechnique of Id: AML.T0016",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Software Tools",
        "code": "AML.T0016.001",
        "details": "<p>Adversaries may search for and obtain software tools to support their operations.<br/>Software designed for legitimate use may be repurposed by an adversary for malicious intent.<br/>An adversary may modify or customize software tools to achieve their purpose.<br/>Software tools used to support attacks on ML systems are not necessarily ML-based themselves.</p>",
        "sort_order": 14
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Develop Capabilities",
        "code": "AML.T0017",
        "details": "<p>Adversaries may develop their own capabilities to support operations. This process encompasses identifying requirements, building solutions, and deploying capabilities. Capabilities used to support attacks on ML systems are not necessarily ML-based themselves. Examples include setting up websites with adversarial information or creating Jupyter notebooks with obfuscated exfiltration code.</p>",
        "sort_order": 15
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Adversarial ML Attack Implementations",
                "details": "<p>Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Develop Capabilities",
            "Subtechnique of Id: AML.T0017",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Adversarial ML Attacks",
        "code": "AML.T0017.000",
        "details": "<p>Adversaries may develop their own adversarial attacks.<br/>They may leverage existing libraries as a starting point (Adversarial ML Attack Implementations).<br/>They may implement ideas described in public research papers or develop custom made attacks for the victim model.</p>",
        "sort_order": 16
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Acquire Infrastructure",
        "code": "AML.T0008",
        "details": "<p>Adversaries may buy, lease, or rent infrastructure for use throughout their operation.<br/>A wide variety of infrastructure exists for hosting and orchestrating adversary operations.<br/>Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services.<br/>Free resources may also be used, but they are typically limited.<br/>Infrastructure can also include physical components such as countermeasures that degrade or disrupt AI components or sensors, including printed materials, wearables, or disguises.</p><p>Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation.<br/>Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services.<br/>Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.</p>",
        "sort_order": 17
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Acquire Infrastructure",
            "Subtechnique of Id: AML.T0008",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "ML Development Workspaces",
        "code": "AML.T0008.000",
        "details": "<p>Developing and staging machine learning attacks often requires expensive compute resources.<br/>Adversaries may need access to one or many GPUs in order to develop an attack.<br/>They may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations.<br/>Multiple workspaces may be used to avoid detection.</p>",
        "sort_order": 18
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Acquire Infrastructure",
            "Subtechnique of Id: AML.T0008",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Consumer Hardware",
        "code": "AML.T0008.001",
        "details": "<p>Adversaries may acquire consumer hardware to conduct their attacks.<br/>Owning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace.</p>",
        "sort_order": 19
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Poison Training Data",
                "details": "<p>Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.<br/>This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.<br/>Data poisoning attacks may or may not require modifying the labels.<br/>The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger</p><p>Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system.</p>"
            },
            {
                "title": "ML Supply Chain Compromise",
                "details": "<p>Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.<br/>This could include Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself.<br/>In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/",
            "Mitigation Name: Verify ML Artifacts",
            "Mitigation Id: AML.M0014",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: AI Bill of Materials",
            "Mitigation Id: AML.M0023",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Policy"
        ],
        "testcase": "Publish Poisoned Datasets",
        "code": "AML.T0019",
        "details": "<p>Adversaries may Poison Training Data and publish it to a public location.<br/>The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.<br/>This data may be introduced to a victim system via ML Supply Chain Compromise.</p><p><b>MITIGATION</b>: Verify the cryptographic checksum of all machine learning artifacts to verify that the file was not modified by an attacker.</p><p>Determine validity of published data in order to avoid using poisoned data that introduces vulnerabilities.</p><p><b>MITIGATION</b>: An AI Bill of Materials (AI BOM) contains a full listing of artifacts and resources that were used in building the AI. The AI BOM can help mitigate supply chain risks and enable rapid response to reported vulnerabilities.</p><p>This can include maintaining dataset provenance, i.e. a detailed history of datasets used for AI applications. The history can include information about the dataset source as well as well as a complete record of any modifications.</p><p>An AI BOM can help users identify untrustworthy model artifacts.</p>",
        "sort_order": 20
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Hardware",
                "details": "<p>Adversaries may target AI systems by disrupting or manipulating the hardware supply chain. AI models often run on specialized hardware such as GPUs, TPUs, or embedded devices, but may also be optimized to operate on CPUs.</p>"
            },
            {
                "title": "Data",
                "details": "<p>Data is a key vector of supply chain compromise for adversaries.<br/>Every machine learning project will require some form of data.<br/>Many rely on large open source datasets that are publicly available.<br/>An adversary could rely on compromising these sources of data.<br/>The malicious data could be a result of Poison Training Data or include traditional malware.</p><p>An adversary can also target private datasets in the labeling phase.<br/>The creation of private datasets will often require the hiring of outside labeling services.<br/>An adversary can poison a dataset by modifying the labels being generated by the labeling service.</p>"
            },
            {
                "title": "ML Software",
                "details": "<p>Most machine learning systems rely on a limited set of machine learning frameworks.<br/>An adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains.<br/>Many machine learning projects also rely on other open source implementations of various algorithms.<br/>These can also be compromised in a targeted way to get access to specific systems.</p>"
            },
            {
                "title": "Model",
                "details": "<p>Machine learning systems often rely on open sourced models in various ways.<br/>Most commonly, the victim organization may be using these models for fine tuning.<br/>These models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset.<br/>Loading models often requires executing some saved code in the form of a saved model file.<br/>These can be compromised with traditional malware, or through some adversarial machine learning techniques.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/",
            "Mitigation Name: Verify ML Artifacts",
            "Mitigation Id: AML.M0014",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Generative AI Guardrails",
            "Mitigation Id: AML.M0020",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML"
        ],
        "testcase": "ML Supply Chain Compromise",
        "code": "AML.T0010",
        "details": "<p>Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.<br/>This could include Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself.<br/>In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.</p><p><b>MITIGATION</b>: Verify the cryptographic checksum of all machine learning artifacts to verify that the file was not modified by an attacker.</p><p>Introduce proper checking of signatures to ensure that unsafe ML artifacts will not be introduced to the system.</p><p><b>MITIGATION</b>: Guardrails are safety controls that are placed between a generative AI model and the output shared with the user to prevent undesired inputs and outputs.<br/>Guardrails can take the form of validators such as filters, rule-based logic, or regular expressions, as well as AI-based approaches, such as classifiers and utilizing LLMs, or named entity recognition (NER) to evaluate the safety of the prompt or response. Domain specific methods can be employed to reduce risks in a variety of areas such as etiquette, brand damage, jailbreaking, false information, code exploits, SQL injections, and data leakage.</p><p>Guardrails can detect harmful code in model outputs.</p>",
        "sort_order": 21
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: ML Supply Chain Compromise",
            "Subtechnique of Id: AML.T0010",
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/"
        ],
        "testcase": "Hardware",
        "code": "AML.T0010.000",
        "details": "<p>Adversaries may target AI systems by disrupting or manipulating the hardware supply chain. AI models often run on specialized hardware such as GPUs, TPUs, or embedded devices, but may also be optimized to operate on CPUs.</p>",
        "sort_order": 22
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: ML Supply Chain Compromise",
            "Subtechnique of Id: AML.T0010",
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/",
            "Mitigation Name: Use Ensemble Methods",
            "Mitigation Id: AML.M0006",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Code Signing",
            "Mitigation Id: AML.M0013",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "ML Software",
        "code": "AML.T0010.001",
        "details": "<p>Most machine learning systems rely on a limited set of machine learning frameworks.<br/>An adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains.<br/>Many machine learning projects also rely on other open source implementations of various algorithms.<br/>These can also be compromised in a targeted way to get access to specific systems.</p><p><b>MITIGATION</b>: Use an ensemble of models for inference to increase robustness to adversarial inputs. Some attacks may effectively evade one model or model family but be ineffective against others.</p><p>Using multiple different models ensures minimal performance loss if security flaw is found in tool for one model or family.</p><p><b>MITIGATION</b>: Enforce binary and application integrity with digital signature verification to prevent untrusted code from executing. Adversaries can embed malicious code in ML software or models. Enforcement of code signing can prevent the compromise of the machine learning supply chain and prevent execution of malicious code.</p><p>Enforce properly signed drivers and ML software frameworks.</p>",
        "sort_order": 23
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Poison Training Data",
                "details": "<p>Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.<br/>This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.<br/>Data poisoning attacks may or may not require modifying the labels.<br/>The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger</p><p>Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: ML Supply Chain Compromise",
            "Subtechnique of Id: AML.T0010",
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/",
            "Mitigation Name: Control Access to ML Models and Data at Rest",
            "Mitigation Id: AML.M0005",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Policy",
            "Mitigation Name: Sanitize Training Data",
            "Mitigation Id: AML.M0007",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Maintain AI Dataset Provenance",
            "Mitigation Id: AML.M0025"
        ],
        "testcase": "Data",
        "code": "AML.T0010.002",
        "details": "<p>Data is a key vector of supply chain compromise for adversaries.<br/>Every machine learning project will require some form of data.<br/>Many rely on large open source datasets that are publicly available.<br/>An adversary could rely on compromising these sources of data.<br/>The malicious data could be a result of Poison Training Data or include traditional malware.</p><p>An adversary can also target private datasets in the labeling phase.<br/>The creation of private datasets will often require the hiring of outside labeling services.<br/>An adversary can poison a dataset by modifying the labels being generated by the labeling service.</p><p><b>MITIGATION</b>: Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.</p><p>Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.</p><p><b>MITIGATION</b>: Detect and remove or remediate poisoned training data.  Training data should be sanitized prior to model training and recurrently for an active learning model.</p><p>Implement a filter to limit ingested training data.  Establish a content policy that would remove unwanted content such as certain explicit or offensive language from being used.</p><p>Detect and remove or remediate poisoned data to avoid adversarial model drift or backdoor attacks.</p><p><b>MITIGATION</b>: Maintain a detailed history of datasets used for AI applications. The history should include information about the dataset's source as well as a complete record of any modifications.</p><p>Dataset provenance can protect against supply chain compromise of data.</p>",
        "sort_order": 24
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: ML Supply Chain Compromise",
            "Subtechnique of Id: AML.T0010",
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/",
            "Mitigation Name: Control Access to ML Models and Data at Rest",
            "Mitigation Id: AML.M0005",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Policy",
            "Mitigation Name: Use Ensemble Methods",
            "Mitigation Id: AML.M0006",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Validate ML Model",
            "Mitigation Id: AML.M0008",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Name: Code Signing",
            "Mitigation Id: AML.M0013",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Model Distribution Methods",
            "Mitigation Id: AML.M0017"
        ],
        "testcase": "Model",
        "code": "AML.T0010.003",
        "details": "<p>Machine learning systems often rely on open sourced models in various ways.<br/>Most commonly, the victim organization may be using these models for fine tuning.<br/>These models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset.<br/>Loading models often requires executing some saved code in the form of a saved model file.<br/>These can be compromised with traditional malware, or through some adversarial machine learning techniques.</p><p><b>MITIGATION</b>: Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.</p><p>Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.</p><p><b>MITIGATION</b>: Use an ensemble of models for inference to increase robustness to adversarial inputs. Some attacks may effectively evade one model or model family but be ineffective against others.</p><p>Using multiple different models ensures minimal performance loss if security flaw is found in tool for one model or family.</p><p><b>MITIGATION</b>: Validate that machine learning models perform as intended by testing for backdoor triggers or adversarial bias.<br/>Monitor model for concept drift and training data drift, which may indicate data tampering and poisoning.</p><p>Ensure that acquired models do not respond to potential backdoor triggers or adversarial bias.</p><p><b>MITIGATION</b>: Enforce binary and application integrity with digital signature verification to prevent untrusted code from executing. Adversaries can embed malicious code in ML software or models. Enforcement of code signing can prevent the compromise of the machine learning supply chain and prevent execution of malicious code.</p><p>Enforce properly signed model files.</p><p><b>MITIGATION</b>: Deploying ML models to edge devices can increase the attack surface of the system.<br/>Consider serving models in the cloud to reduce the level of access the adversary has to the model.<br/>Also consider computing features in the cloud to prevent gray-box attacks, where an adversary has access to the model preprocessing methods.</p><p>An adversary could repackage the application with a malicious version of the model.</p>",
        "sort_order": 25
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Discover ML Model Ontology",
                "details": "<p>Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect.<br/>The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space.<br/>Or the ontology may be discovered in a configuration file or in documentation about the model.</p><p>The model ontology helps the adversary understand how the model is being used by the victim.<br/>It is useful to the adversary in creating targeted attacks.</p>"
            },
            {
                "title": "Discover ML Model Family",
                "details": "<p>Adversaries may discover the general family of model.<br/>General information about the model may be revealed in documentation, or the adversary may use carefully constructed examples and analyze the model's responses to categorize it.</p><p>Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack.</p>"
            },
            {
                "title": "Verify Attack",
                "details": "<p>Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.<br/>This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.<br/>The adversary may verify the attack once but use it against many edge devices running copies of the target model.<br/>The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time.<br/>Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.</p>"
            },
            {
                "title": "Craft Adversarial Data",
                "details": "<p>Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.<br/>Effects can range from misclassification, to missed detections, to maximizing energy consumption.<br/>Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.<br/>For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.</p><p>Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization, Black-Box Optimization, Black-Box Transfer, or Manual Modification.</p><p>The adversary may Verify Attack their approach works if they have white-box or inference API access to the model.<br/>This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.<br/>They can then use the attack at a later time to accomplish their goals.<br/>An adversary may optimize adversarial examples for Evade ML Model, or to Erode ML Model Integrity.</p>"
            },
            {
                "title": "Evade ML Model",
                "details": "<p>Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data.<br/>This technique can be used to evade a downstream task where machine learning is utilized.<br/>The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.</p>"
            },
            {
                "title": "Erode ML Model Integrity",
                "details": "<p>Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.<br/>This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: ML Model Access",
            "Tactic Id: AML.TA0000",
            "Mitigation Name: Control Access to ML Models and Data in Production",
            "Mitigation Id: AML.M0019",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Policy",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "AI Model Inference API Access",
        "code": "AML.T0040",
        "details": "<p>Adversaries may gain access to a model via legitimate access to the inference API.<br/>Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).</p><p>Many systems rely on the same models provided via an inference API, which means they share the same vulnerabilities. This is especially true of foundation models which are prohibitively resource intensive to train. Adversaries may use their access to model APIs to identify vulnerabilities such as jailbreaks or hallucinations and then target applications that use the same models.</p><p><b>MITIGATION</b>: Require users to verify their identities before accessing a production model.<br/>Require authentication for API endpoints and monitor production model queries to ensure compliance with usage policies and to prevent model misuse.</p><p>Adversaries can use unrestricted API access to gain information about a production system, stage attacks, and introduce malicious data to the system.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help audit API usage of the model.</p>",
        "sort_order": 26
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: ML Model Access",
            "Tactic Id: AML.TA0000",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "ML-Enabled Product or Service",
        "code": "AML.T0047",
        "details": "<p>Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model.<br/>This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if sensitive model information has been sent to an attacker.</p>",
        "sort_order": 27
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: ML Model Access",
            "Tactic Id: AML.TA0000",
            "Mitigation Name: Use Multi-Modal Sensors",
            "Mitigation Id: AML.M0009",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Physical Environment Access",
        "code": "AML.T0041",
        "details": "<p>In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.<br/>If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.<br/>By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.</p><p><b>MITIGATION</b>: Incorporate multiple sensors to integrate varying perspectives and modalities to avoid a single point of failure susceptible to physical attacks.</p><p>Using a variety of sensors can make it more difficult for an attacker with physical access to compromise and produce malicious results.</p>",
        "sort_order": 28
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Craft Adversarial Data",
                "details": "<p>Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.<br/>Effects can range from misclassification, to missed detections, to maximizing energy consumption.<br/>Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.<br/>For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.</p><p>Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization, Black-Box Optimization, Black-Box Transfer, or Manual Modification.</p><p>The adversary may Verify Attack their approach works if they have white-box or inference API access to the model.<br/>This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.<br/>They can then use the attack at a later time to accomplish their goals.<br/>An adversary may optimize adversarial examples for Evade ML Model, or to Erode ML Model Integrity.</p>"
            },
            {
                "title": "Verify Attack",
                "details": "<p>Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.<br/>This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.<br/>The adversary may verify the attack once but use it against many edge devices running copies of the target model.<br/>The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time.<br/>Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: ML Model Access",
            "Tactic Id: AML.TA0000",
            "Mitigation Name: Model Distribution Methods",
            "Mitigation Id: AML.M0017",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Policy"
        ],
        "testcase": "Full ML Model Access",
        "code": "AML.T0044",
        "details": "<p>Adversaries may gain full \"white-box\" access to a machine learning model.<br/>This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology.<br/>They may exfiltrate the model to Craft Adversarial Data and Verify Attack in an offline where it is hard to detect their behavior.</p><p><b>MITIGATION</b>: Deploying ML models to edge devices can increase the attack surface of the system.<br/>Consider serving models in the cloud to reduce the level of access the adversary has to the model.<br/>Also consider computing features in the cloud to prevent gray-box attacks, where an adversary has access to the model preprocessing methods.</p><p>Not distributing the model in software to edge devices, can limit an adversary's ability to gain full access to the model.</p>",
        "sort_order": 29
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/",
            "Mitigation Name: Passive ML Output Obfuscation",
            "Mitigation Id: AML.M0002",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Discover ML Model Ontology",
        "code": "AML.T0013",
        "details": "<p>Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect.<br/>The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space.<br/>Or the ontology may be discovered in a configuration file or in documentation about the model.</p><p>The model ontology helps the adversary understand how the model is being used by the victim.<br/>It is useful to the adversary in creating targeted attacks.</p><p><b>MITIGATION</b>: Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.</p><p>Suggested approaches:<br/>  - Restrict the number of results shown<br/>  - Limit specificity of output class ontology<br/>  - Use randomized smoothing techniques<br/>  - Reduce the precision of numerical outputs</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the amount of information an attacker can learn about a model's ontology through API queries.</p>",
        "sort_order": 30
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/",
            "Mitigation Name: Passive ML Output Obfuscation",
            "Mitigation Id: AML.M0002",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Use Ensemble Methods",
            "Mitigation Id: AML.M0006",
            "Mitigation ML Lifecycle: ML Model Engineering"
        ],
        "testcase": "Discover ML Model Family",
        "code": "AML.T0014",
        "details": "<p>Adversaries may discover the general family of model.<br/>General information about the model may be revealed in documentation, or the adversary may use carefully constructed examples and analyze the model's responses to categorize it.</p><p>Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack.</p><p><b>MITIGATION</b>: Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.</p><p>Suggested approaches:<br/>  - Restrict the number of results shown<br/>  - Limit specificity of output class ontology<br/>  - Use randomized smoothing techniques<br/>  - Reduce the precision of numerical outputs</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the amount of information an attacker can learn about a model's ontology through API queries.</p><p><b>MITIGATION</b>: Use an ensemble of models for inference to increase robustness to adversarial inputs. Some attacks may effectively evade one model or model family but be ineffective against others.</p><p>Use multiple different models to fool adversaries of which type of model is used and how the model used.</p>",
        "sort_order": 31
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Insert Backdoor Trigger",
                "details": "<p>The adversary may add a perceptual trigger into inference data.<br/>The trigger may be imperceptible or non-obvious to humans.<br/>This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model.</p>"
            },
            {
                "title": "ML Supply Chain Compromise",
                "details": "<p>Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.<br/>This could include Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself.<br/>In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/",
            "Tactic Name: Persistence",
            "Tactic Id: AML.TA0006",
            "MITRE ATT&CK Reference: TA0003",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0003/",
            "Mitigation Name: Limit Model Artifact Release",
            "Mitigation Id: AML.M0001",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Policy",
            "Mitigation Name: Control Access to ML Models and Data at Rest",
            "Mitigation Id: AML.M0005",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Name: Sanitize Training Data",
            "Mitigation Id: AML.M0007",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: AI Bill of Materials",
            "Mitigation Id: AML.M0023",
            "Mitigation Name: Maintain AI Dataset Provenance",
            "Mitigation Id: AML.M0025"
        ],
        "testcase": "Poison Training Data",
        "code": "AML.T0020",
        "details": "<p>Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels.<br/>This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable.<br/>Data poisoning attacks may or may not require modifying the labels.<br/>The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger</p><p>Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system.</p><p><b>MITIGATION</b>: Limit public release of technical project details including data, algorithms, model architectures, and model checkpoints that are used in production, or that are representative of those used in production.</p><p>Published datasets can be a target for poisoning attacks.</p><p><b>MITIGATION</b>: Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.</p><p>Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.</p><p><b>MITIGATION</b>: Detect and remove or remediate poisoned training data.  Training data should be sanitized prior to model training and recurrently for an active learning model.</p><p>Implement a filter to limit ingested training data.  Establish a content policy that would remove unwanted content such as certain explicit or offensive language from being used.</p><p>Detect modification of data and labels which may cause adversarial model drift or backdoor attacks.</p><p><b>MITIGATION</b>: An AI Bill of Materials (AI BOM) contains a full listing of artifacts and resources that were used in building the AI. The AI BOM can help mitigate supply chain risks and enable rapid response to reported vulnerabilities.</p><p>This can include maintaining dataset provenance, i.e. a detailed history of datasets used for AI applications. The history can include information about the dataset source as well as well as a complete record of any modifications.</p><p>An AI BOM can help users identify untrustworthy model artifacts.</p><p><b>MITIGATION</b>: Maintain a detailed history of datasets used for AI applications. The history should include information about the dataset's source as well as a complete record of any modifications.</p><p>Dataset provenance can protect against poisoning of training data</p>",
        "sort_order": 32
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Establish Accounts",
        "code": "AML.T0021",
        "details": "<p>Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging, or for victim impersonation.</p>",
        "sort_order": 33
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Create Proxy ML Model",
        "code": "AML.T0005",
        "details": "<p>Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.<br/>Proxy models are used to simulate complete access to the target model in a fully offline manner.</p><p>Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.</p>",
        "sort_order": 34
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Create Proxy ML Model",
            "Subtechnique of Id: AML.T0005",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Train Proxy via Gathered ML Artifacts",
        "code": "AML.T0005.000",
        "details": "<p>Proxy models may be trained from ML artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary.<br/>This can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model.</p>",
        "sort_order": 35
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "AI Model Inference API Access",
                "details": "<p>Adversaries may gain access to a model via legitimate access to the inference API.<br/>Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).</p><p>Many systems rely on the same models provided via an inference API, which means they share the same vulnerabilities. This is especially true of foundation models which are prohibitively resource intensive to train. Adversaries may use their access to model APIs to identify vulnerabilities such as jailbreaks or hallucinations and then target applications that use the same models.</p>"
            },
            {
                "title": "Craft Adversarial Data",
                "details": "<p>Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.<br/>Effects can range from misclassification, to missed detections, to maximizing energy consumption.<br/>Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.<br/>For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.</p><p>Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization, Black-Box Optimization, Black-Box Transfer, or Manual Modification.</p><p>The adversary may Verify Attack their approach works if they have white-box or inference API access to the model.<br/>This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.<br/>They can then use the attack at a later time to accomplish their goals.<br/>An adversary may optimize adversarial examples for Evade ML Model, or to Erode ML Model Integrity.</p>"
            },
            {
                "title": "Evade ML Model",
                "details": "<p>Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data.<br/>This technique can be used to evade a downstream task where machine learning is utilized.<br/>The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.</p>"
            },
            {
                "title": "Spamming ML System with Chaff Data",
                "details": "<p>Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.<br/>This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Create Proxy ML Model",
            "Subtechnique of Id: AML.T0005",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Train Proxy via Replication",
        "code": "AML.T0005.001",
        "details": "<p>Adversaries may replicate a private model.<br/>By repeatedly querying the victim's AI Model Inference API Access, the adversary can collect the target model's inferences into a dataset.<br/>The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.</p><p>A replicated model that closely mimic's the target model is a valuable resource in staging the attack.<br/>The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model, Spamming ML System with Chaff Data).</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if a proxy training dataset has been exfiltrated.</p>",
        "sort_order": 36
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Create Proxy ML Model",
            "Subtechnique of Id: AML.T0005",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Use Pre-Trained Model",
        "code": "AML.T0005.002",
        "details": "<p>Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack.</p>",
        "sort_order": 37
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/",
            "Mitigation Name: Encrypt Sensitive Information",
            "Mitigation Id: AML.M0012",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Discover ML Artifacts",
        "code": "AML.T0007",
        "details": "<p>Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them.<br/>These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos.</p><p>This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks.</p><p><b>MITIGATION</b>: Encrypt sensitive data such as ML models to protect against adversaries attempting to access sensitive data.</p><p>Protect machine learning artifacts from adversaries who gather private information to target and improve attacks.</p>",
        "sort_order": 38
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "ML Supply Chain Compromise",
                "details": "<p>Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.<br/>This could include Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself.<br/>In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/",
            "Mitigation Name: User Training",
            "Mitigation Id: AML.M0018",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Policy"
        ],
        "testcase": "User Execution",
        "code": "AML.T0011",
        "details": "<p>An adversary may rely upon specific actions by a user in order to gain execution.<br/>Users may inadvertently execute unsafe code introduced via ML Supply Chain Compromise.<br/>Users may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link.</p><p><b>MITIGATION</b>: Educate ML model developers on secure coding practices and ML vulnerabilities.</p><p>Training users to be able to identify attempts at manipulation will make them less susceptible to performing techniques that cause the execution of malicious code.</p>",
        "sort_order": 39
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "ML Supply Chain Compromise",
                "details": "<p>Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.<br/>This could include Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself.<br/>In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: User Execution",
            "Subtechnique of Id: AML.T0011",
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/",
            "Mitigation Name: Restrict Library Loading",
            "Mitigation Id: AML.M0011",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Code Signing",
            "Mitigation Id: AML.M0013",
            "Mitigation Name: Verify ML Artifacts",
            "Mitigation Id: AML.M0014",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation Name: Vulnerability Scanning",
            "Mitigation Id: AML.M0016",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Name: User Training",
            "Mitigation Id: AML.M0018",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Policy",
            "Mitigation Name: AI Bill of Materials",
            "Mitigation Id: AML.M0023"
        ],
        "testcase": "Unsafe ML Artifacts",
        "code": "AML.T0011.000",
        "details": "<p>Adversaries may develop unsafe ML artifacts that when executed have a deleterious effect.<br/>The adversary can use this technique to establish persistent access to systems.<br/>These models may be introduced via a ML Supply Chain Compromise.</p><p>Serialization of models is a popular technique for model storage, transfer, and loading.<br/>However, this format without proper checking presents an opportunity for code execution.</p><p><b>MITIGATION</b>: Prevent abuse of library loading mechanisms in the operating system and software to load untrusted code by configuring appropriate library loading mechanisms and investigating potential vulnerable software.</p><p>File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for loading of malicious libraries.</p><p>Restrict library loading by ML artifacts.</p><p><b>MITIGATION</b>: Enforce binary and application integrity with digital signature verification to prevent untrusted code from executing. Adversaries can embed malicious code in ML software or models. Enforcement of code signing can prevent the compromise of the machine learning supply chain and prevent execution of malicious code.</p><p>Prevent execution of ML artifacts that are not properly signed.</p><p><b>MITIGATION</b>: Verify the cryptographic checksum of all machine learning artifacts to verify that the file was not modified by an attacker.</p><p>Introduce proper checking of signatures to ensure that unsafe ML artifacts will not be executed in the system.</p><p><b>MITIGATION</b>: Vulnerability scanning is used to find potentially exploitable software vulnerabilities to remediate them.</p><p>File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for arbitrary code execution.<br/>Both model artifacts and downstream products produced by models should be scanned for known vulnerabilities.</p><p>Scan ML artifacts for vulnerabilities before execution.</p><p><b>MITIGATION</b>: Educate ML model developers on secure coding practices and ML vulnerabilities.</p><p>Train users to identify attempts of manipulation to prevent them from running unsafe code which when executed could develop unsafe artifacts. These artifacts may have a detrimental effect on the system.</p><p><b>MITIGATION</b>: An AI Bill of Materials (AI BOM) contains a full listing of artifacts and resources that were used in building the AI. The AI BOM can help mitigate supply chain risks and enable rapid response to reported vulnerabilities.</p><p>This can include maintaining dataset provenance, i.e. a detailed history of datasets used for AI applications. The history can include information about the dataset source as well as well as a complete record of any modifications.</p><p>An AI BOM can help users identify untrustworthy model artifacts.</p>",
        "sort_order": 40
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Discover ML Artifacts",
                "details": "<p>Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them.<br/>These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos.</p><p>This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/"
        ],
        "testcase": "Valid Accounts",
        "code": "AML.T0012",
        "details": "<p>Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access.<br/>Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services.</p><p>Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform Discover ML Artifacts.<br/>Compromised credentials may also grant an adversary increased privileges such as write access to ML artifacts used during development or production.</p>",
        "sort_order": 41
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Craft Adversarial Data",
                "details": "<p>Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.<br/>Effects can range from misclassification, to missed detections, to maximizing energy consumption.<br/>Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.<br/>For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.</p><p>Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization, Black-Box Optimization, Black-Box Transfer, or Manual Modification.</p><p>The adversary may Verify Attack their approach works if they have white-box or inference API access to the model.<br/>This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.<br/>They can then use the attack at a later time to accomplish their goals.<br/>An adversary may optimize adversarial examples for Evade ML Model, or to Erode ML Model Integrity.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/",
            "Tactic Name: Defense Evasion",
            "Tactic Id: AML.TA0007",
            "MITRE ATT&CK Reference: TA0005",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0005/",
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/",
            "Mitigation Name: Model Hardening",
            "Mitigation Id: AML.M0003",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Use Ensemble Methods",
            "Mitigation Id: AML.M0006",
            "Mitigation Name: Use Multi-Modal Sensors",
            "Mitigation Id: AML.M0009",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Input Restoration",
            "Mitigation Id: AML.M0010",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Name: Adversarial Input Detection",
            "Mitigation Id: AML.M0015"
        ],
        "testcase": "Evade ML Model",
        "code": "AML.T0015",
        "details": "<p>Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data.<br/>This technique can be used to evade a downstream task where machine learning is utilized.<br/>The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.</p><p><b>MITIGATION</b>: Use techniques to make machine learning models robust to adversarial inputs such as adversarial training or network distillation.</p><p>Hardened models are more difficult to evade.</p><p><b>MITIGATION</b>: Use an ensemble of models for inference to increase robustness to adversarial inputs. Some attacks may effectively evade one model or model family but be ineffective against others.</p><p>Using multiple different models increases robustness to attack.</p><p><b>MITIGATION</b>: Incorporate multiple sensors to integrate varying perspectives and modalities to avoid a single point of failure susceptible to physical attacks.</p><p>Using a variety of sensors can make it more difficult for an attacker to compromise and produce malicious results.</p><p><b>MITIGATION</b>: Preprocess all inference data to nullify or reverse potential adversarial perturbations.</p><p>Preprocessing model inputs can prevent malicious data from going through the machine learning pipeline.</p><p><b>MITIGATION</b>: Detect and block adversarial inputs or atypical queries that deviate from known benign behavior, exhibit behavior patterns observed in previous attacks or that come from potentially malicious IPs.<br/>Incorporate adversarial detection algorithms into the ML system prior to the ML model.</p><p>Prevent an attacker from introducing adversarial data into the system.</p>",
        "sort_order": 42
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Insert Backdoor Trigger",
                "details": "<p>The adversary may add a perceptual trigger into inference data.<br/>The trigger may be imperceptible or non-obvious to humans.<br/>This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Persistence",
            "Tactic Id: AML.TA0006",
            "MITRE ATT&CK Reference: TA0003",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0003/",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001",
            "Mitigation Name: Vulnerability Scanning",
            "Mitigation Id: AML.M0016",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Backdoor ML Model",
        "code": "AML.T0018",
        "details": "<p>Adversaries may introduce a backdoor into a ML model.<br/>A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data.<br/>A backdoored model provides the adversary with a persistent artifact on the victim system.<br/>The embedded vulnerability is typically activated at a later time by data samples with an Insert Backdoor Trigger</p><p><b>MITIGATION</b>: Vulnerability scanning is used to find potentially exploitable software vulnerabilities to remediate them.</p><p>File formats such as pickle files that are commonly used to store machine learning models can contain exploits that allow for arbitrary code execution.<br/>Both model artifacts and downstream products produced by models should be scanned for known vulnerabilities.</p><p>Techniques such as neural payload injection can make model artifacts vulnerable to adversarial queries. Scan model artifacts for signs of compromise.</p>",
        "sort_order": 43
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Backdoor ML Model",
            "Subtechnique of Id: AML.T0018",
            "Tactic Name: Persistence",
            "Tactic Id: AML.TA0006",
            "MITRE ATT&CK Reference: TA0003",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0003/",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001",
            "Mitigation Name: Control Access to ML Models and Data at Rest",
            "Mitigation Id: AML.M0005",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Policy",
            "Mitigation Name: Sanitize Training Data",
            "Mitigation Id: AML.M0007",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Validate ML Model",
            "Mitigation Id: AML.M0008",
            "Mitigation Name: Maintain AI Dataset Provenance",
            "Mitigation Id: AML.M0025"
        ],
        "testcase": "Poison ML Model",
        "code": "AML.T0018.000",
        "details": "<p>Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process.<br/>The model learns to associate an adversary-defined trigger with the adversary's desired output.</p><p><b>MITIGATION</b>: Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.</p><p>Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.</p><p><b>MITIGATION</b>: Detect and remove or remediate poisoned training data.  Training data should be sanitized prior to model training and recurrently for an active learning model.</p><p>Implement a filter to limit ingested training data.  Establish a content policy that would remove unwanted content such as certain explicit or offensive language from being used.</p><p>Prevent attackers from leveraging poisoned datasets to launch backdoor attacks against a model.</p><p><b>MITIGATION</b>: Validate that machine learning models perform as intended by testing for backdoor triggers or adversarial bias.<br/>Monitor model for concept drift and training data drift, which may indicate data tampering and poisoning.</p><p>Ensure that trained models do not respond to potential backdoor triggers or adversarial bias.</p><p><b>MITIGATION</b>: Maintain a detailed history of datasets used for AI applications. The history should include information about the dataset's source as well as a complete record of any modifications.</p><p>Dataset provenance can protect against poisoning of models.</p>",
        "sort_order": 44
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Backdoor ML Model",
            "Subtechnique of Id: AML.T0018",
            "Tactic Name: Persistence",
            "Tactic Id: AML.TA0006",
            "MITRE ATT&CK Reference: TA0003",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0003/",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001",
            "Mitigation Name: Control Access to ML Models and Data at Rest",
            "Mitigation Id: AML.M0005",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Policy",
            "Mitigation Name: Validate ML Model",
            "Mitigation Id: AML.M0008",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - ML"
        ],
        "testcase": "Inject Payload",
        "code": "AML.T0018.001",
        "details": "<p>Adversaries may introduce a backdoor into a model by injecting a payload into the model file.<br/>The payload detects the presence of the trigger and bypasses the model, instead producing the adversary's desired output.</p><p><b>MITIGATION</b>: Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.</p><p>Access controls can prevent tampering with ML artifacts and prevent unauthorized copying.</p><p><b>MITIGATION</b>: Validate that machine learning models perform as intended by testing for backdoor triggers or adversarial bias.<br/>Monitor model for concept drift and training data drift, which may indicate data tampering and poisoning.</p><p>Ensure that acquired models do not respond to potential backdoor triggers or adversarial bias.</p>",
        "sort_order": 45
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "AI Model Inference API Access",
                "details": "<p>Adversaries may gain access to a model via legitimate access to the inference API.<br/>Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).</p><p>Many systems rely on the same models provided via an inference API, which means they share the same vulnerabilities. This is especially true of foundation models which are prohibitively resource intensive to train. Adversaries may use their access to model APIs to identify vulnerabilities such as jailbreaks or hallucinations and then target applications that use the same models.</p>"
            },
            {
                "title": "Infer Training Data Membership",
                "details": "<p>Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns.<br/>Some strategies make use of a shadow model that could be obtained via Train Proxy via Replication, others use statistics of model prediction scores.</p><p>This can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP.</p>"
            },
            {
                "title": "Invert ML Model",
                "details": "<p>Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API.<br/>By querying the inference API strategically, adversaries can back out potentially private information embedded within the training data.<br/>This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm.</p>"
            },
            {
                "title": "Extract ML Model",
                "details": "<p>Adversaries may extract a functional copy of a private model.<br/>By repeatedly querying the victim's AI Model Inference API Access, the adversary can collect the target model's inferences into a dataset.<br/>The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.</p><p>Adversaries may extract the model to avoid paying per query in a machine learning as a service setting.<br/>Model extraction is used for ML Intellectual Property Theft.</p>"
            },
            {
                "title": "ML Intellectual Property Theft",
                "details": "<p>Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.</p><p>Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft.</p><p>MLaaS providers charge for use of their API.<br/>An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Exfiltration",
            "Tactic Id: AML.TA0010",
            "MITRE ATT&CK Reference: TA0010",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0010/",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Control Access to ML Models and Data in Production",
            "Mitigation Id: AML.M0019",
            "Mitigation Category: Policy",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024"
        ],
        "testcase": "Exfiltration via ML Inference API",
        "code": "AML.T0024",
        "details": "<p>Adversaries may exfiltrate private information via AI Model Inference API Access.<br/>ML Models have been shown leak private information about their training data (e.g.  Infer Training Data Membership, Invert ML Model).<br/>The model itself may also be extracted (Extract ML Model) for the purposes of ML Intellectual Property Theft.</p><p>Exfiltration of information relating to private training data raises privacy concerns.<br/>Private training data may include personally identifiable information, or other protected data.</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.</p><p><b>MITIGATION</b>: Require users to verify their identities before accessing a production model.<br/>Require authentication for API endpoints and monitor production model queries to ensure compliance with usage policies and to prevent model misuse.</p><p>Adversaries can use unrestricted API access to build a proxy training dataset and reveal private information.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if sensitive data has been exfiltrated.</p>",
        "sort_order": 46
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Train Proxy via Replication",
                "details": "<p>Adversaries may replicate a private model.<br/>By repeatedly querying the victim's AI Model Inference API Access, the adversary can collect the target model's inferences into a dataset.<br/>The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.</p><p>A replicated model that closely mimic's the target model is a valuable resource in staging the attack.<br/>The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model, Spamming ML System with Chaff Data).</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Exfiltration via ML Inference API",
            "Subtechnique of Id: AML.T0024",
            "Tactic Name: Exfiltration",
            "Tactic Id: AML.TA0010",
            "MITRE ATT&CK Reference: TA0010",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0010/",
            "Mitigation Name: Passive ML Output Obfuscation",
            "Mitigation Id: AML.M0002",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024"
        ],
        "testcase": "Infer Training Data Membership",
        "code": "AML.T0024.000",
        "details": "<p>Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns.<br/>Some strategies make use of a shadow model that could be obtained via Train Proxy via Replication, others use statistics of model prediction scores.</p><p>This can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP.</p><p><b>MITIGATION</b>: Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.</p><p>Suggested approaches:<br/>  - Restrict the number of results shown<br/>  - Limit specificity of output class ontology<br/>  - Use randomized smoothing techniques<br/>  - Reduce the precision of numerical outputs</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if sensitive data has been exfiltrated.</p>",
        "sort_order": 47
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Exfiltration via ML Inference API",
            "Subtechnique of Id: AML.T0024",
            "Tactic Name: Exfiltration",
            "Tactic Id: AML.TA0010",
            "MITRE ATT&CK Reference: TA0010",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0010/",
            "Mitigation Name: Passive ML Output Obfuscation",
            "Mitigation Id: AML.M0002",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024"
        ],
        "testcase": "Invert ML Model",
        "code": "AML.T0024.001",
        "details": "<p>Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API.<br/>By querying the inference API strategically, adversaries can back out potentially private information embedded within the training data.<br/>This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm.</p><p><b>MITIGATION</b>: Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.</p><p>Suggested approaches:<br/>  - Restrict the number of results shown<br/>  - Limit specificity of output class ontology<br/>  - Use randomized smoothing techniques<br/>  - Reduce the precision of numerical outputs</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if sensitive data has been exfiltrated.</p>",
        "sort_order": 48
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "AI Model Inference API Access",
                "details": "<p>Adversaries may gain access to a model via legitimate access to the inference API.<br/>Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).</p><p>Many systems rely on the same models provided via an inference API, which means they share the same vulnerabilities. This is especially true of foundation models which are prohibitively resource intensive to train. Adversaries may use their access to model APIs to identify vulnerabilities such as jailbreaks or hallucinations and then target applications that use the same models.</p>"
            },
            {
                "title": "ML Intellectual Property Theft",
                "details": "<p>Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.</p><p>Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft.</p><p>MLaaS providers charge for use of their API.<br/>An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Exfiltration via ML Inference API",
            "Subtechnique of Id: AML.T0024",
            "Tactic Name: Exfiltration",
            "Tactic Id: AML.TA0010",
            "MITRE ATT&CK Reference: TA0010",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0010/",
            "Mitigation Name: Passive ML Output Obfuscation",
            "Mitigation Id: AML.M0002",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024"
        ],
        "testcase": "Extract ML Model",
        "code": "AML.T0024.002",
        "details": "<p>Adversaries may extract a functional copy of a private model.<br/>By repeatedly querying the victim's AI Model Inference API Access, the adversary can collect the target model's inferences into a dataset.<br/>The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.</p><p>Adversaries may extract the model to avoid paying per query in a machine learning as a service setting.<br/>Model extraction is used for ML Intellectual Property Theft.</p><p><b>MITIGATION</b>: Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.</p><p>Suggested approaches:<br/>  - Restrict the number of results shown<br/>  - Limit specificity of output class ontology<br/>  - Use randomized smoothing techniques<br/>  - Reduce the precision of numerical outputs</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the volume of API queries in a given period of time to regulate the amount and fidelity of potentially sensitive information an attacker can learn.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if sensitive data has been exfiltrated.</p>",
        "sort_order": 49
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Exfiltration",
            "Tactic Id: AML.TA0010",
            "MITRE ATT&CK Reference: TA0010",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0010/",
            "Mitigation Name: Control Access to ML Models and Data at Rest",
            "Mitigation Id: AML.M0005",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Policy"
        ],
        "testcase": "Exfiltration via Cyber Means",
        "code": "AML.T0025",
        "details": "<p>Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means.</p><p>See the ATT&CK <a href=\"https://attack.mitre.org/tactics/TA0010/\" rel=\"noopener noreferrer\" target=\"_blank\">Exfiltration</a> tactic for more information.</p><p><b>MITIGATION</b>: Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.</p><p>Access controls can prevent exfiltration.</p>",
        "sort_order": 50
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Adversarial Input Detection",
            "Mitigation Id: AML.M0015",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Technical - ML"
        ],
        "testcase": "Denial of ML Service",
        "code": "AML.T0029",
        "details": "<p>Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service.<br/>Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.<br/>Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the number of queries users can perform in a given interval to prevent a denial of service.</p><p><b>MITIGATION</b>: Detect and block adversarial inputs or atypical queries that deviate from known benign behavior, exhibit behavior patterns observed in previous attacks or that come from potentially malicious IPs.<br/>Incorporate adversarial detection algorithms into the ML system prior to the ML model.</p><p>Assess queries before inference call or enforce timeout policy for queries which consume excessive resources.</p>",
        "sort_order": 51
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Spamming ML System with Chaff Data",
        "code": "AML.T0046",
        "details": "<p>Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections.<br/>This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the number of queries users can perform in a given interval to protect the system from chaff data spam.</p>",
        "sort_order": 52
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/",
            "Mitigation Name: Model Hardening",
            "Mitigation Id: AML.M0003",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Use Ensemble Methods",
            "Mitigation Id: AML.M0006",
            "Mitigation Name: Input Restoration",
            "Mitigation Id: AML.M0010",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Name: Adversarial Input Detection",
            "Mitigation Id: AML.M0015"
        ],
        "testcase": "Erode ML Model Integrity",
        "code": "AML.T0031",
        "details": "<p>Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.<br/>This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.</p><p><b>MITIGATION</b>: Use techniques to make machine learning models robust to adversarial inputs such as adversarial training or network distillation.</p><p>Hardened models are less susceptible to integrity attacks.</p><p><b>MITIGATION</b>: Use an ensemble of models for inference to increase robustness to adversarial inputs. Some attacks may effectively evade one model or model family but be ineffective against others.</p><p>Using multiple different models increases robustness to attack.</p><p><b>MITIGATION</b>: Preprocess all inference data to nullify or reverse potential adversarial perturbations.</p><p>Preprocessing model inputs can prevent malicious data from going through the machine learning pipeline.</p><p><b>MITIGATION</b>: Detect and block adversarial inputs or atypical queries that deviate from known benign behavior, exhibit behavior patterns observed in previous attacks or that come from potentially malicious IPs.<br/>Incorporate adversarial detection algorithms into the ML system prior to the ML model.</p><p>Incorporate adversarial input detection into the pipeline before inputs reach the model.</p>",
        "sort_order": 53
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Cost Harvesting",
        "code": "AML.T0034",
        "details": "<p>Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.<br/>Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the number of queries users can perform in a given interval to hinder an attacker's ability to send computationally expensive inputs</p>",
        "sort_order": 54
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Collection",
            "Tactic Id: AML.TA0009",
            "MITRE ATT&CK Reference: TA0009",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0009/",
            "Mitigation Name: Encrypt Sensitive Information",
            "Mitigation Id: AML.M0012",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "ML Artifact Collection",
        "code": "AML.T0035",
        "details": "<p>Adversaries may collect ML artifacts for Exfiltration or for use in ML Attack Staging.<br/>ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model.</p><p><b>MITIGATION</b>: Encrypt sensitive data such as ML models to protect against adversaries attempting to access sensitive data.</p><p>Protect machine learning artifacts with encryption.</p>",
        "sort_order": 55
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Collection",
            "Tactic Id: AML.TA0009",
            "MITRE ATT&CK Reference: TA0009",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0009/"
        ],
        "testcase": "Data from Information Repositories",
        "code": "AML.T0036",
        "details": "<p>Adversaries may leverage information repositories to mine valuable information.<br/>Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information.</p><p>Information stored in a repository may vary based on the specific instance or environment.<br/>Specific common information repositories include SharePoint, Confluence, and enterprise databases such as SQL Server.</p>",
        "sort_order": 56
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Collection",
            "Tactic Id: AML.TA0009",
            "MITRE ATT&CK Reference: TA0009",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0009/"
        ],
        "testcase": "Data from Local System",
        "code": "AML.T0037",
        "details": "<p>Adversaries may search local system sources, such as file systems and configuration files or local databases, to find files of interest and sensitive data prior to Exfiltration.</p><p>This can include basic fingerprinting information and sensitive data such as ssh keys.</p>",
        "sort_order": 57
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Physical Environment Access",
                "details": "<p>In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks.<br/>If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected.<br/>By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Verify Attack",
        "code": "AML.T0042",
        "details": "<p>Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.<br/>This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.<br/>The adversary may verify the attack once but use it against many edge devices running copies of the target model.<br/>The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time.<br/>Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.</p>",
        "sort_order": 58
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "White-Box Optimization",
                "details": "<p>In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly.<br/>Adversarial examples trained in this manner are most effective against the target model.</p>"
            },
            {
                "title": "Black-Box Optimization",
                "details": "<p>In Black-Box attacks, the adversary has black-box (i.e. AI Model Inference API Access via API access) access to the target model.<br/>With black-box attacks, the adversary may be using an API that the victim is monitoring.<br/>These attacks are generally less effective and require more inferences than White-Box Optimization attacks, but they require much less access.</p>"
            },
            {
                "title": "Black-Box Transfer",
                "details": "<p>In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via Create Proxy ML Model or Train Proxy via Replication) they have full access to and are representative of the target model.<br/>The adversary uses White-Box Optimization on the proxy models to generate adversarial examples.<br/>If the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another.<br/>This means that an attack that works for the proxy models will likely then work for the target model.<br/>If the adversary has AI Model Inference API Access, they may use Verify Attack to confirm the attack is working and incorporate that information into their training process.</p>"
            },
            {
                "title": "Manual Modification",
                "details": "<p>Adversaries may manually modify the input data to craft adversarial data.<br/>They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task.<br/>The adversary may use trial and error until they are able to verify they have a working adversarial input.</p>"
            },
            {
                "title": "Verify Attack",
                "details": "<p>Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.<br/>This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.<br/>The adversary may verify the attack once but use it against many edge devices running copies of the target model.<br/>The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time.<br/>Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.</p>"
            },
            {
                "title": "Evade ML Model",
                "details": "<p>Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data.<br/>This technique can be used to evade a downstream task where machine learning is utilized.<br/>The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.</p>"
            },
            {
                "title": "Erode ML Model Integrity",
                "details": "<p>Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.<br/>This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Craft Adversarial Data",
        "code": "AML.T0043",
        "details": "<p>Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model.<br/>Effects can range from misclassification, to missed detections, to maximizing energy consumption.<br/>Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect.<br/>For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class.</p><p>Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization, Black-Box Optimization, Black-Box Transfer, or Manual Modification.</p><p>The adversary may Verify Attack their approach works if they have white-box or inference API access to the model.<br/>This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed.<br/>They can then use the attack at a later time to accomplish their goals.<br/>An adversary may optimize adversarial examples for Evade ML Model, or to Erode ML Model Integrity.</p>",
        "sort_order": 59
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Craft Adversarial Data",
            "Subtechnique of Id: AML.T0043",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001",
            "Mitigation Name: Model Distribution Methods",
            "Mitigation Id: AML.M0017",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Policy"
        ],
        "testcase": "White-Box Optimization",
        "code": "AML.T0043.000",
        "details": "<p>In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly.<br/>Adversarial examples trained in this manner are most effective against the target model.</p><p><b>MITIGATION</b>: Deploying ML models to edge devices can increase the attack surface of the system.<br/>Consider serving models in the cloud to reduce the level of access the adversary has to the model.<br/>Also consider computing features in the cloud to prevent gray-box attacks, where an adversary has access to the model preprocessing methods.</p><p>With full access to the model, an adversary could perform white-box attacks.</p>",
        "sort_order": 60
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "AI Model Inference API Access",
                "details": "<p>Adversaries may gain access to a model via legitimate access to the inference API.<br/>Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).</p><p>Many systems rely on the same models provided via an inference API, which means they share the same vulnerabilities. This is especially true of foundation models which are prohibitively resource intensive to train. Adversaries may use their access to model APIs to identify vulnerabilities such as jailbreaks or hallucinations and then target applications that use the same models.</p>"
            },
            {
                "title": "White-Box Optimization",
                "details": "<p>In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly.<br/>Adversarial examples trained in this manner are most effective against the target model.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Craft Adversarial Data",
            "Subtechnique of Id: AML.T0043",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001",
            "Mitigation Name: Passive ML Output Obfuscation",
            "Mitigation Id: AML.M0002",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Restrict Number of ML Model Queries",
            "Mitigation Id: AML.M0004",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber",
            "Mitigation Name: Input Restoration",
            "Mitigation Id: AML.M0010",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation Name: Adversarial Input Detection",
            "Mitigation Id: AML.M0015",
            "Mitigation ML Lifecycle: ML Model Engineering"
        ],
        "testcase": "Black-Box Optimization",
        "code": "AML.T0043.001",
        "details": "<p>In Black-Box attacks, the adversary has black-box (i.e. AI Model Inference API Access via API access) access to the target model.<br/>With black-box attacks, the adversary may be using an API that the victim is monitoring.<br/>These attacks are generally less effective and require more inferences than White-Box Optimization attacks, but they require much less access.</p><p><b>MITIGATION</b>: Decreasing the fidelity of model outputs provided to the end user can reduce an adversaries ability to extract information about the model and optimize attacks for the model.</p><p>Suggested approaches:<br/>  - Restrict the number of results shown<br/>  - Limit specificity of output class ontology<br/>  - Use randomized smoothing techniques<br/>  - Reduce the precision of numerical outputs</p><p><b>MITIGATION</b>: Limit the total number and rate of queries a user can perform.</p><p>Limit the number of queries users can perform in a given interval to shrink the attack surface for black-box attacks.</p><p><b>MITIGATION</b>: Preprocess all inference data to nullify or reverse potential adversarial perturbations.</p><p>Input restoration adds an extra layer of unknowns and randomness when an adversary evaluates the input-output relationship.</p><p><b>MITIGATION</b>: Detect and block adversarial inputs or atypical queries that deviate from known benign behavior, exhibit behavior patterns observed in previous attacks or that come from potentially malicious IPs.<br/>Incorporate adversarial detection algorithms into the ML system prior to the ML model.</p><p>Monitor queries and query patterns to the target model, block access if suspicious queries are detected.</p>",
        "sort_order": 61
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Create Proxy ML Model",
                "details": "<p>Adversaries may obtain models to serve as proxies for the target model in use at the victim organization.<br/>Proxy models are used to simulate complete access to the target model in a fully offline manner.</p><p>Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.</p>"
            },
            {
                "title": "Train Proxy via Replication",
                "details": "<p>Adversaries may replicate a private model.<br/>By repeatedly querying the victim's AI Model Inference API Access, the adversary can collect the target model's inferences into a dataset.<br/>The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.</p><p>A replicated model that closely mimic's the target model is a valuable resource in staging the attack.<br/>The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model, Spamming ML System with Chaff Data).</p>"
            },
            {
                "title": "White-Box Optimization",
                "details": "<p>In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly.<br/>Adversarial examples trained in this manner are most effective against the target model.</p>"
            },
            {
                "title": "AI Model Inference API Access",
                "details": "<p>Adversaries may gain access to a model via legitimate access to the inference API.<br/>Inference API access can be a source of information to the adversary (Discover ML Model Ontology, Discover ML Model Family), a means of staging the attack (Verify Attack, Craft Adversarial Data), or for introducing data to the target system for Impact (Evade ML Model, Erode ML Model Integrity).</p><p>Many systems rely on the same models provided via an inference API, which means they share the same vulnerabilities. This is especially true of foundation models which are prohibitively resource intensive to train. Adversaries may use their access to model APIs to identify vulnerabilities such as jailbreaks or hallucinations and then target applications that use the same models.</p>"
            },
            {
                "title": "Verify Attack",
                "details": "<p>Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model.<br/>This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing.<br/>The adversary may verify the attack once but use it against many edge devices running copies of the target model.<br/>The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time.<br/>Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Craft Adversarial Data",
            "Subtechnique of Id: AML.T0043",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Black-Box Transfer",
        "code": "AML.T0043.002",
        "details": "<p>In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via Create Proxy ML Model or Train Proxy via Replication) they have full access to and are representative of the target model.<br/>The adversary uses White-Box Optimization on the proxy models to generate adversarial examples.<br/>If the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another.<br/>This means that an attack that works for the proxy models will likely then work for the target model.<br/>If the adversary has AI Model Inference API Access, they may use Verify Attack to confirm the attack is working and incorporate that information into their training process.</p>",
        "sort_order": 62
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Craft Adversarial Data",
            "Subtechnique of Id: AML.T0043",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Manual Modification",
        "code": "AML.T0043.003",
        "details": "<p>Adversaries may manually modify the input data to craft adversarial data.<br/>They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task.<br/>The adversary may use trial and error until they are able to verify they have a working adversarial input.</p>",
        "sort_order": 63
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Poison ML Model",
                "details": "<p>Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process.<br/>The model learns to associate an adversary-defined trigger with the adversary's desired output.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Craft Adversarial Data",
            "Subtechnique of Id: AML.T0043",
            "Tactic Name: ML Attack Staging",
            "Tactic Id: AML.TA0001"
        ],
        "testcase": "Insert Backdoor Trigger",
        "code": "AML.T0043.004",
        "details": "<p>The adversary may add a perceptual trigger into inference data.<br/>The trigger may be imperceptible or non-obvious to humans.<br/>This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model.</p>",
        "sort_order": 64
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/"
        ],
        "testcase": "External Harms",
        "code": "AML.T0048",
        "details": "<p>Adversaries may abuse their access to a victim system and use its resources or capabilities to further their goals by causing harms external to that system.<br/>These harms could affect the organization (e.g. Financial Harm, Reputational Harm), its users (e.g. User Harm), or the general public (e.g. Societal Harm).</p>",
        "sort_order": 65
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: External Harms",
            "Subtechnique of Id: AML.T0048",
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/"
        ],
        "testcase": "Financial Harm",
        "code": "AML.T0048.000",
        "details": "<p>Financial harm involves the loss of wealth, property, or other monetary assets due to theft, fraud or forgery, or pressure to provide financial resources to the adversary.</p>",
        "sort_order": 66
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: External Harms",
            "Subtechnique of Id: AML.T0048",
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/"
        ],
        "testcase": "Reputational Harm",
        "code": "AML.T0048.001",
        "details": "<p>Reputational harm involves a degradation of public perception and trust in organizations.  Examples of reputation-harming incidents include scandals or false impersonations.</p>",
        "sort_order": 67
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: External Harms",
            "Subtechnique of Id: AML.T0048",
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/"
        ],
        "testcase": "Societal Harm",
        "code": "AML.T0048.002",
        "details": "<p>Societal harms might generate harmful outcomes that reach either the general public or specific vulnerable groups such as the exposure of children to vulgar content.</p>",
        "sort_order": 68
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: External Harms",
            "Subtechnique of Id: AML.T0048",
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/"
        ],
        "testcase": "User Harm",
        "code": "AML.T0048.003",
        "details": "<p>User harms may encompass a variety of harm types including financial and reputational that are directed at or felt by individual victims of the attack rather than at the organization level.</p>",
        "sort_order": 69
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Extract ML Model",
                "details": "<p>Adversaries may extract a functional copy of a private model.<br/>By repeatedly querying the victim's AI Model Inference API Access, the adversary can collect the target model's inferences into a dataset.<br/>The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model.</p><p>Adversaries may extract the model to avoid paying per query in a machine learning as a service setting.<br/>Model extraction is used for ML Intellectual Property Theft.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: External Harms",
            "Subtechnique of Id: AML.T0048",
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/",
            "Mitigation Name: Control Access to ML Models and Data at Rest",
            "Mitigation Id: AML.M0005",
            "Mitigation ML Lifecycle: Business and Data Understanding",
            "Mitigation ML Lifecycle: Data Preparation",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: ML Model Evaluation",
            "Mitigation Category: Policy",
            "Mitigation Name: Encrypt Sensitive Information",
            "Mitigation Id: AML.M0012",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "ML Intellectual Property Theft",
        "code": "AML.T0048.004",
        "details": "<p>Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization.</p><p>Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft.</p><p>MLaaS providers charge for use of their API.<br/>An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property.</p><p><b>MITIGATION</b>: Establish access controls on internal model registries and limit internal access to production models. Limit access to training data only to approved users.</p><p>Access controls can prevent theft of intellectual property.</p><p><b>MITIGATION</b>: Encrypt sensitive data such as ML models to protect against adversaries attempting to access sensitive data.</p><p>Protect machine learning artifacts with encryption.</p>",
        "sort_order": 70
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/"
        ],
        "testcase": "Exploit Public-Facing Application",
        "code": "AML.T0049",
        "details": "<p>Adversaries may attempt to take advantage of a weakness in an Internet-facing computer or program using software, data, or commands in order to cause unintended or unanticipated behavior. The weakness in the system can be a bug, a glitch, or a design vulnerability. These applications are often websites, but can include databases (like SQL), standard services (like SMB or SSH), network device administration and management protocols (like SNMP and Smart Install), and any other applications with Internet accessible open sockets, such as web servers and related services.</p>",
        "sort_order": 71
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/"
        ],
        "testcase": "Command and Scripting Interpreter",
        "code": "AML.T0050",
        "details": "<p>Adversaries may abuse command and script interpreters to execute commands, scripts, or binaries. These interfaces and languages provide ways of interacting with computer systems and are a common feature across many different platforms. Most systems come with some built-in command-line interface and scripting capabilities, for example, macOS and Linux distributions include some flavor of Unix Shell while Windows installations include the Windows Command Shell and PowerShell.</p><p>There are also cross-platform interpreters such as Python, as well as those commonly associated with client applications such as JavaScript and Visual Basic.</p><p>Adversaries may abuse these technologies in various ways as a means of executing arbitrary commands. Commands and scripts can be embedded in Initial Access payloads delivered to victims as lure documents or as secondary payloads downloaded from an existing C2. Adversaries may also execute commands through interactive terminals/shells, as well as utilize various Remote Services in order to achieve remote Execution.</p>",
        "sort_order": 72
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Direct",
                "details": "<p>An adversary may inject prompts directly as a user of the LLM. This type of injection may be used by the adversary to gain a foothold in the system or to misuse the LLM itself, as for example to generate harmful content.</p>"
            },
            {
                "title": "Indirect",
                "details": "<p>An adversary may inject prompts indirectly via separate data channel ingested by the LLM such as include text or multimedia pulled from databases or websites.<br/>These malicious prompts may be hidden or obfuscated from the user. This type of injection may be used by the adversary to gain a foothold in the system or to target an unwitting user of the system.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/",
            "Mitigation Name: Generative AI Guardrails",
            "Mitigation Id: AML.M0020",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Generative AI Guidelines",
            "Mitigation Id: AML.M0021",
            "Mitigation Name: Generative AI Model Alignment",
            "Mitigation Id: AML.M0022",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "LLM Prompt Injection",
        "code": "AML.T0051",
        "details": "<p>An adversary may craft malicious prompts as inputs to an LLM that cause the LLM to act in unintended ways.<br/>These \"prompt injections\" are often designed to cause the model to ignore aspects of its original instructions and follow the adversary's instructions instead.</p><p>Prompt Injections can be an initial access vector to the LLM that provides the adversary with a foothold to carry out other steps in their operation.<br/>They may be designed to bypass defenses in the LLM, or allow the adversary to issue privileged commands.<br/>The effects of a prompt injection can persist throughout an interactive session with an LLM.</p><p>Malicious prompts may be injected directly by the adversary (Direct) either to leverage the LLM to generate harmful content or to gain a foothold on the system and lead to further effects.<br/>Prompts may also be injected indirectly when as part of its normal operation the LLM ingests the malicious prompt from another data source (Indirect). This type of injection can be used by the adversary to a foothold on the system or to target the user of the LLM.</p><p><b>MITIGATION</b>: Guardrails are safety controls that are placed between a generative AI model and the output shared with the user to prevent undesired inputs and outputs.<br/>Guardrails can take the form of validators such as filters, rule-based logic, or regular expressions, as well as AI-based approaches, such as classifiers and utilizing LLMs, or named entity recognition (NER) to evaluate the safety of the prompt or response. Domain specific methods can be employed to reduce risks in a variety of areas such as etiquette, brand damage, jailbreaking, false information, code exploits, SQL injections, and data leakage.</p><p>Guardrails can prevent harmful inputs that can lead to prompt injection.</p><p><b>MITIGATION</b>: Guidelines are safety controls that are placed between user-provided input and a generative AI model to help direct the model to produce desired outputs and prevent undesired outputs.</p><p>Guidelines can be implemented as instructions appended to all user prompts or as part of the instructions in the system prompt. They can define the goal(s), role, and voice of the system, as well as outline safety and security parameters.</p><p>Model guidelines can instruct the model to refuse a response to unsafe inputs.</p><p><b>MITIGATION</b>: When training or fine-tuning a generative AI model it is important to utilize techniques that improve model alignment with safety, security, and content policies.</p><p>The fine-tuning process can potentially remove built-in safety mechanisms in a generative AI model, but utilizing techniques such as Supervised Fine-Tuning, Reinforcement Learning from Human Feedback or AI Feedback, and Targeted Safety Context Distillation can improve the safety and alignment of the model.</p><p>Model alignment can improve the parametric safety of a model by guiding it away from unsafe prompts and responses.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if unsafe prompts have been submitted to the LLM.</p>",
        "sort_order": 73
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: LLM Prompt Injection",
            "Subtechnique of Id: AML.T0051",
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Direct",
        "code": "AML.T0051.000",
        "details": "<p>An adversary may inject prompts directly as a user of the LLM. This type of injection may be used by the adversary to gain a foothold in the system or to misuse the LLM itself, as for example to generate harmful content.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if unsafe prompts have been submitted to the LLM.</p>",
        "sort_order": 74
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: LLM Prompt Injection",
            "Subtechnique of Id: AML.T0051",
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/",
            "Mitigation Name: AI Telemetry Logging",
            "Mitigation Id: AML.M0024",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation Category: Technical - Cyber"
        ],
        "testcase": "Indirect",
        "code": "AML.T0051.001",
        "details": "<p>An adversary may inject prompts indirectly via separate data channel ingested by the LLM such as include text or multimedia pulled from databases or websites.<br/>These malicious prompts may be hidden or obfuscated from the user. This type of injection may be used by the adversary to gain a foothold in the system or to target an unwitting user of the system.</p><p><b>MITIGATION</b>: Implement logging of inputs and outputs of deployed AI models. Monitoring logs can help to detect security threats and mitigate impacts.</p><p>Telemetry logging can help identify if unsafe prompts have been submitted to the LLM.</p>",
        "sort_order": 75
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/"
        ],
        "testcase": "Phishing",
        "code": "AML.T0052",
        "details": "<p>Adversaries may send phishing messages to gain access to victim systems. All forms of phishing are electronically delivered social engineering. Phishing can be targeted, known as spearphishing. In spearphishing, a specific individual, company, or industry will be targeted by the adversary. More generally, adversaries can conduct non-targeted phishing, such as in mass malware spam campaigns.</p><p>Generative AI, including LLMs that generate synthetic text, visual deepfakes of faces, and audio deepfakes of speech, is enabling adversaries to scale targeted phishing campaigns. LLMs can interact with users via text conversations and can be programmed with a meta prompt to phish for sensitive information. Deepfakes can be use in impersonation as an aid to phishing.</p>",
        "sort_order": 76
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Phishing",
            "Subtechnique of Id: AML.T0052",
            "Tactic Name: Initial Access",
            "Tactic Id: AML.TA0004",
            "MITRE ATT&CK Reference: TA0001",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0001/"
        ],
        "testcase": "Spearphishing via Social Engineering LLM",
        "code": "AML.T0052.000",
        "details": "<p>Adversaries may turn LLMs into targeted social engineers.<br/>LLMs are capable of interacting with users via text conversations.<br/>They can be instructed by an adversary to seek sensitive information from a user and act as effective social engineers.<br/>They can be targeted towards particular personas defined by the adversary.<br/>This allows adversaries to scale spearphishing efforts and target individuals to reveal private information such as credentials to privileged systems.</p>",
        "sort_order": 77
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/",
            "Tactic Name: Privilege Escalation",
            "Tactic Id: AML.TA0012",
            "MITRE ATT&CK Reference: TA0004",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0004/",
            "Mitigation Name: Generative AI Guardrails",
            "Mitigation Id: AML.M0020",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Generative AI Guidelines",
            "Mitigation Id: AML.M0021",
            "Mitigation Name: Generative AI Model Alignment",
            "Mitigation Id: AML.M0022"
        ],
        "testcase": "LLM Plugin Compromise",
        "code": "AML.T0053",
        "details": "<p>Adversaries may use their access to an LLM that is part of a larger system to compromise connected plugins.<br/>LLMs are often connected to other services or resources via plugins to increase their capabilities.<br/>Plugins may include integrations with other applications, access to public or private data sources, and the ability to execute code.</p><p>This may allow adversaries to execute API calls to integrated applications or plugins, providing the adversary with increased privileges on the system.<br/>Adversaries may take advantage of connected data sources to retrieve sensitive information.<br/>They may also use an LLM integrated with a command or script interpreter to execute arbitrary instructions.</p><p><b>MITIGATION</b>: Guardrails are safety controls that are placed between a generative AI model and the output shared with the user to prevent undesired inputs and outputs.<br/>Guardrails can take the form of validators such as filters, rule-based logic, or regular expressions, as well as AI-based approaches, such as classifiers and utilizing LLMs, or named entity recognition (NER) to evaluate the safety of the prompt or response. Domain specific methods can be employed to reduce risks in a variety of areas such as etiquette, brand damage, jailbreaking, false information, code exploits, SQL injections, and data leakage.</p><p>Guardrails can prevent harmful inputs that can lead to plugin compromise, and they can detect PII in model outputs.</p><p><b>MITIGATION</b>: Guidelines are safety controls that are placed between user-provided input and a generative AI model to help direct the model to produce desired outputs and prevent undesired outputs.</p><p>Guidelines can be implemented as instructions appended to all user prompts or as part of the instructions in the system prompt. They can define the goal(s), role, and voice of the system, as well as outline safety and security parameters.</p><p>Model guidelines can instruct the model to refuse a response to unsafe inputs.</p><p><b>MITIGATION</b>: When training or fine-tuning a generative AI model it is important to utilize techniques that improve model alignment with safety, security, and content policies.</p><p>The fine-tuning process can potentially remove built-in safety mechanisms in a generative AI model, but utilizing techniques such as Supervised Fine-Tuning, Reinforcement Learning from Human Feedback or AI Feedback, and Targeted Safety Context Distillation can improve the safety and alignment of the model.</p><p>Model alignment can improve the parametric safety of a model by guiding it away from unsafe prompts and responses.</p>",
        "sort_order": 78
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "LLM Prompt Injection",
                "details": "<p>An adversary may craft malicious prompts as inputs to an LLM that cause the LLM to act in unintended ways.<br/>These \"prompt injections\" are often designed to cause the model to ignore aspects of its original instructions and follow the adversary's instructions instead.</p><p>Prompt Injections can be an initial access vector to the LLM that provides the adversary with a foothold to carry out other steps in their operation.<br/>They may be designed to bypass defenses in the LLM, or allow the adversary to issue privileged commands.<br/>The effects of a prompt injection can persist throughout an interactive session with an LLM.</p><p>Malicious prompts may be injected directly by the adversary (Direct) either to leverage the LLM to generate harmful content or to gain a foothold on the system and lead to further effects.<br/>Prompts may also be injected indirectly when as part of its normal operation the LLM ingests the malicious prompt from another data source (Indirect). This type of injection can be used by the adversary to a foothold on the system or to target the user of the LLM.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Privilege Escalation",
            "Tactic Id: AML.TA0012",
            "MITRE ATT&CK Reference: TA0004",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0004/",
            "Tactic Name: Defense Evasion",
            "Tactic Id: AML.TA0007",
            "MITRE ATT&CK Reference: TA0005",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0005/",
            "Mitigation Name: Generative AI Guardrails",
            "Mitigation Id: AML.M0020",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Generative AI Guidelines",
            "Mitigation Id: AML.M0021",
            "Mitigation Name: Generative AI Model Alignment",
            "Mitigation Id: AML.M0022"
        ],
        "testcase": "LLM Jailbreak",
        "code": "AML.T0054",
        "details": "<p>An adversary may use a carefully crafted LLM Prompt Injection designed to place LLM in a state in which it will freely respond to any user input, bypassing any controls, restrictions, or guardrails placed on the LLM.<br/>Once successfully jailbroken, the LLM can be used in unintended ways by the adversary.</p><p><b>MITIGATION</b>: Guardrails are safety controls that are placed between a generative AI model and the output shared with the user to prevent undesired inputs and outputs.<br/>Guardrails can take the form of validators such as filters, rule-based logic, or regular expressions, as well as AI-based approaches, such as classifiers and utilizing LLMs, or named entity recognition (NER) to evaluate the safety of the prompt or response. Domain specific methods can be employed to reduce risks in a variety of areas such as etiquette, brand damage, jailbreaking, false information, code exploits, SQL injections, and data leakage.</p><p>Guardrails can prevent harmful inputs that can lead to a jailbreak.</p><p><b>MITIGATION</b>: Guidelines are safety controls that are placed between user-provided input and a generative AI model to help direct the model to produce desired outputs and prevent undesired outputs.</p><p>Guidelines can be implemented as instructions appended to all user prompts or as part of the instructions in the system prompt. They can define the goal(s), role, and voice of the system, as well as outline safety and security parameters.</p><p>Model guidelines can instruct the model to refuse a response to unsafe inputs.</p><p><b>MITIGATION</b>: When training or fine-tuning a generative AI model it is important to utilize techniques that improve model alignment with safety, security, and content policies.</p><p>The fine-tuning process can potentially remove built-in safety mechanisms in a generative AI model, but utilizing techniques such as Supervised Fine-Tuning, Reinforcement Learning from Human Feedback or AI Feedback, and Targeted Safety Context Distillation can improve the safety and alignment of the model.</p><p>Model alignment can improve the parametric safety of a model by guiding it away from unsafe prompts and responses.</p>",
        "sort_order": 79
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Credential Access",
            "Tactic Id: AML.TA0013",
            "MITRE ATT&CK Reference: TA0006",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0006/"
        ],
        "testcase": "Unsecured Credentials",
        "code": "AML.T0055",
        "details": "<p>Adversaries may search compromised systems to find and obtain insecurely stored credentials.<br/>These credentials can be stored and/or misplaced in many locations on a system, including plaintext files (e.g. bash history), environment variables, operating system, or application-specific repositories (e.g. Credentials in Registry), or other specialized files/artifacts (e.g. private keys).</p>",
        "sort_order": 80
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Exfiltration",
            "Tactic Id: AML.TA0010",
            "MITRE ATT&CK Reference: TA0010",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0010/",
            "Mitigation Name: Generative AI Guardrails",
            "Mitigation Id: AML.M0020",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Generative AI Guidelines",
            "Mitigation Id: AML.M0021",
            "Mitigation Name: Generative AI Model Alignment",
            "Mitigation Id: AML.M0022"
        ],
        "testcase": "Extract LLM System Prompt",
        "code": "AML.T0056",
        "details": "<p>Adversaries may attempt to extract a large language model's (LLM) system prompt. This can be done via prompt injection to induce the model to reveal its own system prompt or may be extracted from a configuration file.</p><p>System prompts can be a portion of an AI provider's competitive advantage and are thus valuable intellectual property that may be targeted by adversaries.</p><p><b>MITIGATION</b>: Guardrails are safety controls that are placed between a generative AI model and the output shared with the user to prevent undesired inputs and outputs.<br/>Guardrails can take the form of validators such as filters, rule-based logic, or regular expressions, as well as AI-based approaches, such as classifiers and utilizing LLMs, or named entity recognition (NER) to evaluate the safety of the prompt or response. Domain specific methods can be employed to reduce risks in a variety of areas such as etiquette, brand damage, jailbreaking, false information, code exploits, SQL injections, and data leakage.</p><p>Guardrails can prevent harmful inputs that can lead to meta prompt extraction.</p><p><b>MITIGATION</b>: Guidelines are safety controls that are placed between user-provided input and a generative AI model to help direct the model to produce desired outputs and prevent undesired outputs.</p><p>Guidelines can be implemented as instructions appended to all user prompts or as part of the instructions in the system prompt. They can define the goal(s), role, and voice of the system, as well as outline safety and security parameters.</p><p>Model guidelines can instruct the model to refuse a response to unsafe inputs.</p><p><b>MITIGATION</b>: When training or fine-tuning a generative AI model it is important to utilize techniques that improve model alignment with safety, security, and content policies.</p><p>The fine-tuning process can potentially remove built-in safety mechanisms in a generative AI model, but utilizing techniques such as Supervised Fine-Tuning, Reinforcement Learning from Human Feedback or AI Feedback, and Targeted Safety Context Distillation can improve the safety and alignment of the model.</p><p>Model alignment can improve the parametric safety of a model by guiding it away from unsafe prompts and responses.</p>",
        "sort_order": 81
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Exfiltration",
            "Tactic Id: AML.TA0010",
            "MITRE ATT&CK Reference: TA0010",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0010/",
            "Mitigation Name: Generative AI Guardrails",
            "Mitigation Id: AML.M0020",
            "Mitigation ML Lifecycle: ML Model Engineering",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Technical - ML",
            "Mitigation Name: Generative AI Guidelines",
            "Mitigation Id: AML.M0021",
            "Mitigation Name: Generative AI Model Alignment",
            "Mitigation Id: AML.M0022"
        ],
        "testcase": "LLM Data Leakage",
        "code": "AML.T0057",
        "details": "<p>Adversaries may craft prompts that induce the LLM to leak sensitive information.<br/>This can include private user data or proprietary information.<br/>The leaked information may come from proprietary training data, data sources the LLM is connected to, or information from other users of the LLM.</p><p><b>MITIGATION</b>: Guardrails are safety controls that are placed between a generative AI model and the output shared with the user to prevent undesired inputs and outputs.<br/>Guardrails can take the form of validators such as filters, rule-based logic, or regular expressions, as well as AI-based approaches, such as classifiers and utilizing LLMs, or named entity recognition (NER) to evaluate the safety of the prompt or response. Domain specific methods can be employed to reduce risks in a variety of areas such as etiquette, brand damage, jailbreaking, false information, code exploits, SQL injections, and data leakage.</p><p>Guardrails can detect sensitive data and PII in model outputs.</p><p><b>MITIGATION</b>: Guidelines are safety controls that are placed between user-provided input and a generative AI model to help direct the model to produce desired outputs and prevent undesired outputs.</p><p>Guidelines can be implemented as instructions appended to all user prompts or as part of the instructions in the system prompt. They can define the goal(s), role, and voice of the system, as well as outline safety and security parameters.</p><p>Model guidelines can instruct the model to refuse a response to unsafe inputs.</p><p><b>MITIGATION</b>: When training or fine-tuning a generative AI model it is important to utilize techniques that improve model alignment with safety, security, and content policies.</p><p>The fine-tuning process can potentially remove built-in safety mechanisms in a generative AI model, but utilizing techniques such as Supervised Fine-Tuning, Reinforcement Learning from Human Feedback or AI Feedback, and Targeted Safety Context Distillation can improve the safety and alignment of the model.</p><p>Model alignment can improve the parametric safety of a model by guiding it away from unsafe prompts and responses.</p>",
        "sort_order": 82
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "ML Supply Chain Compromise",
                "details": "<p>Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.<br/>This could include Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself.<br/>In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/",
            "Mitigation Name: AI Bill of Materials",
            "Mitigation Id: AML.M0023",
            "Mitigation ML Lifecycle: Monitoring and Maintenance",
            "Mitigation ML Lifecycle: Deployment",
            "Mitigation Category: Policy"
        ],
        "testcase": "Publish Poisoned Models",
        "code": "AML.T0058",
        "details": "<p>Adversaries may publish a poisoned model to a public location such as a model registry or code repository. The poisoned model may be a novel model or a poisoned variant of an existing open-source model. This model may be introduced to a victim system via ML Supply Chain Compromise.</p><p><b>MITIGATION</b>: An AI Bill of Materials (AI BOM) contains a full listing of artifacts and resources that were used in building the AI. The AI BOM can help mitigate supply chain risks and enable rapid response to reported vulnerabilities.</p><p>This can include maintaining dataset provenance, i.e. a detailed history of datasets used for AI applications. The history can include information about the dataset source as well as well as a complete record of any modifications.</p><p>An AI BOM can help users identify untrustworthy model artifacts.</p>",
        "sort_order": 83
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Impact",
            "Tactic Id: AML.TA0011",
            "MITRE ATT&CK Reference: TA0040",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0040/"
        ],
        "testcase": "Erode Dataset Integrity",
        "code": "AML.T0059",
        "details": "<p>Adversaries may poison or manipulate portions of a dataset to reduce its usefulness, reduce trust, and cause users to waste resources correcting errors.</p>",
        "sort_order": 84
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "ML Supply Chain Compromise",
                "details": "<p>Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain.<br/>This could include Hardware, Data and its annotations, parts of the ML ML Software stack, or the Model itself.<br/>In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: User Execution",
            "Subtechnique of Id: AML.T0011",
            "Tactic Name: Execution",
            "Tactic Id: AML.TA0005",
            "MITRE ATT&CK Reference: TA0002",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0002/"
        ],
        "testcase": "Malicious Package",
        "code": "AML.T0011.001",
        "details": "<p>Adversaries may develop malicious software packages that when imported by a user have a deleterious effect.<br/>Malicious packages may behave as expected to the user. They may be introduced via ML Supply Chain Compromise. They may not present as obviously malicious to the user and may appear to be useful for an AI-related task.</p>",
        "sort_order": 85
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Publish Hallucinated Entities",
        "code": "AML.T0060",
        "details": "<p>Adversaries may create an entity they control, such as a software package, website, or email address to a source hallucinated by an LLM. The hallucinations may take the form of package names commands, URLs, company names, or email addresses that point the victim to the entity controlled by the adversary. When the victim interacts with the adversary-controlled entity, the attack can proceed.</p>",
        "sort_order": 86
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "LLM Prompt Injection",
                "details": "<p>An adversary may craft malicious prompts as inputs to an LLM that cause the LLM to act in unintended ways.<br/>These \"prompt injections\" are often designed to cause the model to ignore aspects of its original instructions and follow the adversary's instructions instead.</p><p>Prompt Injections can be an initial access vector to the LLM that provides the adversary with a foothold to carry out other steps in their operation.<br/>They may be designed to bypass defenses in the LLM, or allow the adversary to issue privileged commands.<br/>The effects of a prompt injection can persist throughout an interactive session with an LLM.</p><p>Malicious prompts may be injected directly by the adversary (Direct) either to leverage the LLM to generate harmful content or to gain a foothold on the system and lead to further effects.<br/>Prompts may also be injected indirectly when as part of its normal operation the LLM ingests the malicious prompt from another data source (Indirect). This type of injection can be used by the adversary to a foothold on the system or to target the user of the LLM.</p>"
            },
            {
                "title": "LLM Jailbreak",
                "details": "<p>An adversary may use a carefully crafted LLM Prompt Injection designed to place LLM in a state in which it will freely respond to any user input, bypassing any controls, restrictions, or guardrails placed on the LLM.<br/>Once successfully jailbroken, the LLM can be used in unintended ways by the adversary.</p>"
            },
            {
                "title": "LLM Data Leakage",
                "details": "<p>Adversaries may craft prompts that induce the LLM to leak sensitive information.<br/>This can include private user data or proprietary information.<br/>The leaked information may come from proprietary training data, data sources the LLM is connected to, or information from other users of the LLM.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Persistence",
            "Tactic Id: AML.TA0006",
            "MITRE ATT&CK Reference: TA0003",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0003/"
        ],
        "testcase": "LLM Prompt Self-Replication",
        "code": "AML.T0061",
        "details": "<p>An adversary may use a carefully crafted LLM Prompt Injection designed to cause the LLM to replicate the prompt as part of its output. This allows the prompt to propagate to other LLMs and persist on the system. The self-replicating prompt is typically paired with other malicious instructions (ex: LLM Jailbreak, LLM Data Leakage).</p>",
        "sort_order": 87
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Publish Hallucinated Entities",
                "details": "<p>Adversaries may create an entity they control, such as a software package, website, or email address to a source hallucinated by an LLM. The hallucinations may take the form of package names commands, URLs, company names, or email addresses that point the victim to the entity controlled by the adversary. When the victim interacts with the adversary-controlled entity, the attack can proceed.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/"
        ],
        "testcase": "Discover LLM Hallucinations",
        "code": "AML.T0062",
        "details": "<p>Adversaries may prompt large language models and identify hallucinated entities.<br/>They may request software packages, commands, URLs, organization names, or e-mail addresses, and identify hallucinations with no connected real-world source. Discovered hallucinations provide the adversary with potential targets to Publish Hallucinated Entities. Different LLMs have been shown to produce the same hallucinations, so the hallucinations exploited by an adversary may affect users of other LLMs.</p>",
        "sort_order": 88
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Publish Poisoned Datasets",
                "details": "<p>Adversaries may Poison Training Data and publish it to a public location.<br/>The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset.<br/>This data may be introduced to a victim system via ML Supply Chain Compromise.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Acquire Infrastructure",
            "Subtechnique of Id: AML.T0008",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Domains",
        "code": "AML.T0008.002",
        "details": "<p>Adversaries may acquire domains that can be used during targeting. Domain names are the human readable names used to represent one or more IP addresses. They can be purchased or, in some cases, acquired for free.</p><p>Adversaries may use acquired domains for a variety of purposes (see <a href=\"https://attack.mitre.org/techniques/T1583/001/\" rel=\"noopener noreferrer\" target=\"_blank\">ATT&CK</a>). Large AI datasets are often distributed as a list of URLs to individual datapoints. Adversaries may acquire expired domains that are included in these datasets and replace individual datapoints with poisoned examples (Publish Poisoned Datasets).</p>",
        "sort_order": 89
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Acquire Infrastructure",
            "Subtechnique of Id: AML.T0008",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Physical Countermeasures",
        "code": "AML.T0008.003",
        "details": "<p>Adversaries may acquire or manufacture physical countermeasures to aid or support their attack.</p><p>These components may be used to disrupt or degrade the model, such as adversarial patterns printed on stickers or T-shirts, disguises, or decoys. They may also be used to disrupt or degrade the sensors used in capturing data, such as laser pointers, light bulbs, or other tools.</p>",
        "sort_order": 90
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/"
        ],
        "testcase": "Discover AI Model Outputs",
        "code": "AML.T0063",
        "details": "<p>Adversaries may discover model outputs, such as class scores, whose presence is not required for the system to function and are not intended for use by the end user. Model outputs may be found in logs or may be included in API responses.<br/>Model outputs may enable the adversary to identify weaknesses in the model and develop attacks.</p>",
        "sort_order": 91
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Retrieval Content Crafting",
                "details": "<p>Adversaries may write content designed to be retrieved by user queries and influence a user of the system in some way. This abuses the trust the user has in the system.</p><p>The crafted content can be combined with a prompt injection. It can also stand alone in a separate document or email. The adversary must get the crafted content into the victim\\u0027s database, such as a vector database used in a retrieval augmented generation (RAG) system. This may be accomplished via cyber access, or by abusing the ingestion mechanisms common in RAG systems (see RAG Poisoning).</p><p>Large language models may be used as an assistant to aid an adversary in crafting content.</p>"
            },
            {
                "title": "Phishing",
                "details": "<p>Adversaries may send phishing messages to gain access to victim systems. All forms of phishing are electronically delivered social engineering. Phishing can be targeted, known as spearphishing. In spearphishing, a specific individual, company, or industry will be targeted by the adversary. More generally, adversaries can conduct non-targeted phishing, such as in mass malware spam campaigns.</p><p>Generative AI, including LLMs that generate synthetic text, visual deepfakes of faces, and audio deepfakes of speech, is enabling adversaries to scale targeted phishing campaigns. LLMs can interact with users via text conversations and can be programmed with a meta prompt to phish for sensitive information. Deepfakes can be use in impersonation as an aid to phishing.</p>"
            }
        ],
        "tags":
        [
            "Subtechnique of Name: Obtain Capabilities",
            "Subtechnique of Id: AML.T0016",
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Generative AI",
        "code": "AML.T0016.002",
        "details": "<p>Adversaries may search for and obtain generative AI models or tools, such as large language models (LLMs), to assist them in various steps of their operation. Generative AI can be used in a variety of malicious ways, including generating malware or offensive cyber scripts, Retrieval Content Crafting, or generating Phishing content.</p><p>Adversaries may obtain an open source model or they may leverage a generative AI service. They may need to jailbreak the generative AI model to bypass any restrictions put in place to limit the types of responses it can generate. They may also need to break the terms of service of the generative AI.</p>",
        "sort_order": 92
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Reconnaissance",
            "Tactic Id: AML.TA0002",
            "MITRE ATT&CK Reference: TA0043",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0043/"
        ],
        "testcase": "Gather RAG-Indexed Targets",
        "code": "AML.T0064",
        "details": "<p>Adversaries may identify data sources used in retrieval augmented generation (RAG) systems for targeting purposes. By pinpointing these sources, attackers can focus on poisoning or otherwise manipulating the external data repositories the AI relies on.</p><p>RAG-indexed data may be identified in public documentation about the system, or by interacting with the system directly and observing any indications of or references to external data sources.</p>",
        "sort_order": 93
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "LLM Prompt Crafting",
        "code": "AML.T0065",
        "details": "<p>Adversaries may use their acquired knowledge of the target generative AI system to craft prompts that bypass its defenses and allow malicious instructions to be executed.</p><p>The adversary may iterate on the prompt to ensure that it works as-intended consistently.</p>",
        "sort_order": 94
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "RAG Poisoning",
                "details": "<p>Adversaries may inject malicious content into data indexed by a retrieval augmented generation (RAG) system to contaminate a future thread through RAG-based search results. This may be accomplished by placing manipulated documents in a location the RAG indexes (see Gather RAG-Indexed Targets).</p><p>The content may be targeted such that it would always surface as a search result for a specific user query. The adversary's content may include false or misleading information. It may also include prompt injections with malicious instructions, or false RAG entries.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Resource Development",
            "Tactic Id: AML.TA0003",
            "MITRE ATT&CK Reference: TA0042",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0042/"
        ],
        "testcase": "Retrieval Content Crafting",
        "code": "AML.T0066",
        "details": "<p>Adversaries may write content designed to be retrieved by user queries and influence a user of the system in some way. This abuses the trust the user has in the system.</p><p>The crafted content can be combined with a prompt injection. It can also stand alone in a separate document or email. The adversary must get the crafted content into the victim\\u0027s database, such as a vector database used in a retrieval augmented generation (RAG) system. This may be accomplished via cyber access, or by abusing the ingestion mechanisms common in RAG systems (see RAG Poisoning).</p><p>Large language models may be used as an assistant to aid an adversary in crafting content.</p>",
        "sort_order": 95
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Citations",
                "details": "<p>Adversaries may manipulate the citations provided in an AI system's response, in order to make it appear trustworthy. Variants include citing a providing the wrong citation, making up a new citation, or providing the right citation but for adversary-provided data.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Defense Evasion",
            "Tactic Id: AML.TA0007",
            "MITRE ATT&CK Reference: TA0005",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0005/"
        ],
        "testcase": "LLM Trusted Output Components Manipulation",
        "code": "AML.T0067",
        "details": "<p>Adversaries may utilize prompts to a large language model (LLM) which manipulate various components of its response in order to make it appear trustworthy to the user. This helps the adversary continue to operate in the victim's environment and evade detection by the users it interacts with.</p><p>The LLM may be instructed to tailor its language to appear more trustworthy to the user or attempt to manipulate the user to take certain actions. Other response components that could be manipulated include links, recommended follow-up actions, retrieved document metadata, and Citations.</p>",
        "sort_order": 96
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Defense Evasion",
            "Tactic Id: AML.TA0007",
            "MITRE ATT&CK Reference: TA0005",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0005/"
        ],
        "testcase": "LLM Prompt Obfuscation",
        "code": "AML.T0068",
        "details": "<p>Adversaries may hide or otherwise obfuscate prompt injections or retrieval content from the user to avoid detection.</p><p>This may include modifying how the injection is rendered such as small text, text colored the same as the background, or hidden HTML elements.</p>",
        "sort_order": 97
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/"
        ],
        "testcase": "Discover LLM System Information",
        "code": "AML.T0069",
        "details": "<p>The adversary is trying to discover something about the large language model's (LLM) system information. This may be found in a configuration file containing the system instructions or extracted via interactions with the LLM. The desired information may include the full system prompt, special characters that have significance to the LLM or keywords indicating functionality available to the LLM. Information about how the LLM is instructed can be used by the adversary to understand the system's capabilities and to aid them in crafting malicious prompts.</p>",
        "sort_order": 98
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Discover LLM System Information",
            "Subtechnique of Id: AML.T0069",
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/"
        ],
        "testcase": "Special Character Sets",
        "code": "AML.T0069.000",
        "details": "<p>Adversaries may discover delimiters and special characters sets used by the large language model. For example, delimiters used in retrieval augmented generation applications to differentiate between context and user prompts. These can later be exploited to confuse or manipulate the large language model into misbehaving.</p>",
        "sort_order": 99
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Discover LLM System Information",
            "Subtechnique of Id: AML.T0069",
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/"
        ],
        "testcase": "System Instruction Keywords",
        "code": "AML.T0069.001",
        "details": "<p>Adversaries may discover keywords that have special meaning to the large language model (LLM), such as function names or object names. These can later be exploited to confuse or manipulate the LLM into misbehaving and to make calls to plugins the LLM has access to.</p>",
        "sort_order": 100
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: Discover LLM System Information",
            "Subtechnique of Id: AML.T0069",
            "Tactic Name: Discovery",
            "Tactic Id: AML.TA0008",
            "MITRE ATT&CK Reference: TA0007",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0007/"
        ],
        "testcase": "System Prompt",
        "code": "AML.T0069.002",
        "details": "<p>Adversaries may discover a large language model's system instructions provided by the AI system builder to learn about the system's capabilities and circumvent its guardrails.</p>",
        "sort_order": 101
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [
            {
                "title": "Gather RAG-Indexed Targets",
                "details": "<p>Adversaries may identify data sources used in retrieval augmented generation (RAG) systems for targeting purposes. By pinpointing these sources, attackers can focus on poisoning or otherwise manipulating the external data repositories the AI relies on.</p><p>RAG-indexed data may be identified in public documentation about the system, or by interacting with the system directly and observing any indications of or references to external data sources.</p>"
            }
        ],
        "tags":
        [
            "Tactic Name: Persistence",
            "Tactic Id: AML.TA0006",
            "MITRE ATT&CK Reference: TA0003",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0003/"
        ],
        "testcase": "RAG Poisoning",
        "code": "AML.T0070",
        "details": "<p>Adversaries may inject malicious content into data indexed by a retrieval augmented generation (RAG) system to contaminate a future thread through RAG-based search results. This may be accomplished by placing manipulated documents in a location the RAG indexes (see Gather RAG-Indexed Targets).</p><p>The content may be targeted such that it would always surface as a search result for a specific user query. The adversary's content may include false or misleading information. It may also include prompt injections with malicious instructions, or false RAG entries.</p>",
        "sort_order": 102
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Tactic Name: Defense Evasion",
            "Tactic Id: AML.TA0007",
            "MITRE ATT&CK Reference: TA0005",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0005/"
        ],
        "testcase": "False RAG Entry Injection",
        "code": "AML.T0071",
        "details": "<p>Adversaries may introduce false entries into a victim's retrieval augmented generation (RAG) database. Content designed to be interpreted as a document by the large language model (LLM) used in the RAG system is included in a data source being ingested into the RAG database. When RAG entry including the false document is retrieved the, the LLM is tricked into treating part of the retrieved content as a false RAG result. </p><p>By including a false RAG document inside of a regular RAG entry, it bypasses data monitoring tools. It also prevents the document from being deleted directly. </p><p>The adversary may use discovered system keywords to learn how to instruct a particular LLM to treat content as a RAG entry. They may be able to manipulate the injected entry's metadata including document title, author, and creation date.</p>",
        "sort_order": 103
    },
    {
        "custom_fields":
        [],
        "execution_flow":
        [],
        "tags":
        [
            "Subtechnique of Name: LLM Trusted Output Components Manipulation",
            "Subtechnique of Id: AML.T0067",
            "Tactic Name: Defense Evasion",
            "Tactic Id: AML.TA0007",
            "MITRE ATT&CK Reference: TA0005",
            "MITRE ATT&CK URL: https://attack.mitre.org/tactics/TA0005/"
        ],
        "testcase": "Citations",
        "code": "AML.T0067.000",
        "details": "<p>Adversaries may manipulate the citations provided in an AI system's response, in order to make it appear trustworthy. Variants include citing a providing the wrong citation, making up a new citation, or providing the right citation but for adversary-provided data.</p>",
        "sort_order": 104
    }
]
